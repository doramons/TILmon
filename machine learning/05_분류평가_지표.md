#### 모델 평가
  모델의 성능을 평가한다. 평가 결과에 따라 프로세스를 다시 반복한다
 ![image](https://user-images.githubusercontent.com/76146752/112139307-f5bdf180-8c15-11eb-9b29-aa0720e604f6.png)

#### 분류와 회귀의 평가방법
 ##### 분류 평가 지표
   1. 정확도 (Accuracy)
   2. 정밀도 (Precision)
   3. 재현률 (Recall)
   4. F1점수 (F1 Score)
   5. PR Curve, AP
   6. ROC, AUC

 ##### 회귀 평가방법
   1. MSE (Mean Squared Error)
   2. RMSE(Root Mean Squared Error)
   3. R\*\*2 (결정계수)

 ##### sckit-learn 평가함수
  
  - sklearn.metrics 모듈을 통해 제공

### 분류(Classification) 평가 기준
 #### 용어
  - 이진 분류에서 양성과 음성
    - 양성: 예측하려는(찾으려는) 대상
    - 음성: 예측하려는 대상이 아닌 것 
    - 예
        - 암환자 분류: 양성 - 암 환자, 음성 - 정상인
        - 스팸메일 분류: 양성 - 스팸메일, 음성 - 정상메일
        - 금융사기 모델: 양성 - 사기거래, 음성 - 정상거래

 #### 정확도 (Accuracy)
  - 정확도(Accuracy) = 맞게 예측한 건수/ 전체예측건수
  - 전체 예측한 것 중 맞게 예측한 비율로 평가한다
  - `accuracy_score(정답, 모델예측값)`

  ##### Accuracy 평가지표의 문제
   - 불균형 데이터의 경우 정확한 평가지표가 될 수 없다
      - 예: 양성과 음성의 비율이 1:9인 경우 모두 음성이라고 하면 정확도는 90%가 된다

 ### 혼동행렬 (Confusion Matrix)
  - 분류의 평가지표의 기준으로 사용된다
  - 혼동행렬을 이용해 다양한 평가지표(정확도, 재현률, F1 점수, AUC 점수)를 계산할 수 있다
  - 함수: confusion_matrix(정답, 모델예측값)
  - 결과의 0번축: 실제(Ground Truth) class, 1번 축: 예측 class
![image](https://user-images.githubusercontent.com/76146752/112164889-9bcb2500-8c31-11eb-9d76-5e34a6a10db5.png)
![image](https://user-images.githubusercontent.com/76146752/112165088-c74e0f80-8c31-11eb-9d27-7438a996d95b.png)

  - TP(True Positive) - 양성으로 예측했는데 맞은 개수
  - TN(True Negative) - 음성으로 예측했는데 맞은 개수
  - FP(False Positive) - 양성으로 예측했는데 틀린 개수 (음성을 양성으로 예측)
  - FN(False Negative) - 음성으로 예측했는데 틀린 개수 (양성을 음성으로 예측)
 
 #### 이진 분류 평가점수
  
  - Accuracy (정확도)
    - 전체 데이터 중에 맞게 예측한 것의 비율
    - 균형데이터일 때 주로 사용하면 괜찮음

  - Recall/Sensitivity(재현율/민감도)
    - 실제 Positive(양성)인 것 중에 Positive(양성)로 예측한 것의 비율
    - TPR(True Positive Rate) 이라고도 한다
    - ex) 스팸 메일 중 스팸메일로 예측한 비율, 금융 사기 데이터 중 사기로 예측한 비율

  - Precision(정밀도)
    - positive(양성)으로 예측한 것 중 실제 Positive(양성)인 비율
    - **PPV**(Positive Predictive Value)라고도 한다
    - ex) 스팸메일로 예측한 것 중에 스팸메일의 비율 금융 사기로 예측한 것 중 금융사기인 것의 비율

  - F1 점수
    - 정밀도와 재현율의 조화평균 점수
    - recall과 precision이 비슷할수록 높은 값을 가지게 된다 F1 score이 높다는 것은 recall과 precision이 한쪽으로 치우쳐 있지 않고 둘다  좋다고 판단할 수 있는 근거가 된다

 #### 기타
  - Specificity(특이도)
    - 실제 Negative(음성)인 것들 중 Negative(음성)으로 맞게 예측한 것의 비율
    - TNR(True Negative Rate)라고도 한다

  - Fall out(위양성률)
    - 실제 Negative(음성)인 것들 중 Positive(양성)으로 잘못 예측한 것의 비율 `1 - 특이도`
    - FPR (False Positive Rate) 라고도 한다
![image](https://user-images.githubusercontent.com/76146752/112170844-bc49ae00-8c36-11eb-91c5-222467132f50.png)ㅇ

#### 각 평가 지표 계산 함수
  - sklearn.metrics 모듈
  
  - confusion_matrix(y 실제값, x를 넣어 예측한 값)
    - 혼돈 행렬 반환
  - recall_score(y 실제값, x를 넣어 예측한 값)
    - Recall(재현율) 점수 반환 (Positive 중 Positive로 예측한 비율(TPR))

  - f1_score (y 실제값, x를 넣어 예측한 값)
    - F1 점수 반환 (recall과 precision의 조화 평균값)

  - classification_report(y 실제값, x를 넣어 예측한 값)
    - 클래스 별로 recall, precision, f1점수와 accuracy를 종합해서 보여준다

 #### Dummy 모델 혼동행렬
  - 
    ``` python
        from sklearn.metrics import confusion_matrix, plot_confusion_matrix, recall_score, precision_score, f1_score,accuracy_score, classificaition_report
        
        print('Train confusion matrix')
        print(confusion_matrix(y_train, pred_train))
        
        print('Test confusion matrix')
        print(confution_matrix(y_test, pred_test))
        
        from sklearn.dummy import DummyClassifier
        
        dummy_model = DummyClassifier(strategy = 'most_frequent') # 전략 : most_frequent(최빈값)
        dummy_model.fit(X_train, y_train)
        
        fig, ax = plt.subplots(1,1,figsize=(7,7))
        plot_confusion_matrix(dummy_model, # 학습시킨모델
                              X_train, # X
                              y_train, # y
                              display_labels = ['Neg', 'Pos'],
                              cmap = "Blues",
                              ax = ax )
         plt.show()
     ```
   ![image](https://user-images.githubusercontent.com/76146752/112174731-fe282380-8c39-11eb-970f-66d6ca2ac7db.png)
  
  #### dummy 모델 Accuracy, Recall, Precision, F1-Score
  
   - 
     ``` python
         
         print("Accuracy")
         accuracy_score(y_train, pred_train), accuracy_score(y_test, pred_test)
         
         print('Recall') # 9(양성) 중 9로 예측하여 맞은 것
         recall_score(y_train, pred_train), recall_score(y_test,pred_test)
         
         print('Precision') # 양성으로 예측한 것 중 실제로 양성 인것
         precision_score(y_train,pred_train), precision_test(y_test, pred_test)
         
         print('F1-Score')
         f1_score(y_train, pred_train), f1_score(y_test, pred_test)
         
         print('Classification Report')
         print(classification_report(y_train, pred_train), classification(y_test, pred_test))
     ```
     
  #### 재현율과 정밀도의 관계

  이진 분류의 경우 Precision(정밀도)가 중요한 경우와 Recall(재현율)이 중요한 업무가 있다
  
  - 재현율(recall)이 더 중요한 경우
    - 실제 Positive 데이터를 Negative로 잘못 판단하면 업무상 큰 영향이 있는 경우.sh
    - FN(False Negative)를 낮추는데 초점을 맞춘다
    - 암환자 판정 모델, 보험사기적발 모뎅

  - 정밀도가 더 중요한 경우
     - 실제 Negative 데이터를 Positive로 잘못 판단하면 업무상 큰 영향이 있는 경우
     - FP(False Positive)를 낮추는데 초점을 맞춘다
     - 스팸메일 판정

  #### 임계값(Threshold) 변경을 통한 재현율, 정밀도 변환
   - 임계값: 모델이 분류의 답을 결정할 때 기준값
   - 정밀도나 재현율을 특히 강조해야하는 상황일 경우 임계값 변경을 통해 평가 수치를 올릴 수 있다
   - 단 극단적으로 임계점을 올리거나 낮춰서 한쪽 점수만 높이면 안됨 (ex: 암환자 예측시 재현율을 너무 높이면 정밀도가 낮아져 걸핏하면 정상인을 암환자로 예측하게 된다)

  ##### 임계값 변경에 따른 정밀도와 재현율 변화관계
  - 임계값을 높이면 양성으로 예측하는 기준을 높여서 (엄격해짐) 음성으로 예측되는 샘픙이 많아진다. 그래서 정밀도는 높아지고 재현율은 낮아진다
  - 임계값을 낮추면 양성으로 예측하는 기준이 낮아져서 양성으로 예측되는 샘픙이 많아진다 그래서 재현율은 높아지고 정밀도는 낮아진다
    - **임계값을 낮추면 재현율은 올라가고 정밀도는 낮아진다**
    - **임계값을 높이면 재현율은 낮아지고 정밀도는 올라간다**
  - 임계값을 변화시켰을 때 재현율과 정밀도는 음의 상관관계를 가진다
  - 임계값을 변화시켰을때 재현율과 위양성율(Fall-Out/FPR)은 양의 상관관계를 가진다
 ![image](https://user-images.githubusercontent.com/76146752/112181048-6a595600-8c3f-11eb-93c9-05be62d048bf.png)











