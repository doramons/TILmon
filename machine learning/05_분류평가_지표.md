#### 모델 평가
  모델의 성능을 평가한다. 평가 결과에 따라 프로세스를 다시 반복한다
 ![image](https://user-images.githubusercontent.com/76146752/112139307-f5bdf180-8c15-11eb-9b29-aa0720e604f6.png)

#### 분류와 회귀의 평가방법
 ##### 분류 평가 지표
   1. 정확도 (Accuracy)
   2. 정밀도 (Precision)
   3. 재현률 (Recall)
   4. F1점수 (F1 Score)
   5. PR Curve, AP
   6. ROC, AUC

 ##### 회귀 평가방법
   1. MSE (Mean Squared Error)
   2. RMSE(Root Mean Squared Error)
   3. R\*\*2 (결정계수)

 ##### sckit-learn 평가함수
  
  - sklearn.metrics 모듈을 통해 제공

### 분류(Classification) 평가 기준
 #### 용어
  - 이진 분류에서 양성과 음성
    - 양성: 예측하려는(찾으려는) 대상
    - 음성: 예측하려는 대상이 아닌 것 
    - 예
        - 암환자 분류: 양성 - 암 환자, 음성 - 정상인
        - 스팸메일 분류: 양성 - 스팸메일, 음성 - 정상메일
        - 금융사기 모델: 양성 - 사기거래, 음성 - 정상거래

 #### 정확도 (Accuracy)
  - 정확도(Accuracy) = 맞게 예측한 건수/ 전체예측건수
  - 전체 예측한 것 중 맞게 예측한 비율로 평가한다
  - `accuracy_score(정답, 모델예측값)`

  ##### Accuracy 평가지표의 문제
   - 불균형 데이터의 경우 정확한 평가지표가 될 수 없다
      - 예: 양성과 음성의 비율이 1:9인 경우 모두 음성이라고 하면 정확도는 90%가 된다

 ### 혼동행렬 (Confusion Matrix)
  - 분류의 평가지표의 기준으로 사용된다
  - 혼동행렬을 이용해 다양한 평가지표(정확도, 재현률, F1 점수, AUC 점수)를 계산할 수 있다
  - 함수: confusion_matrix(정답, 모델예측값)
  - 결과의 0번축: 실제(Ground Truth) class, 1번 축: 예측 class
![image](https://user-images.githubusercontent.com/76146752/112164889-9bcb2500-8c31-11eb-9d76-5e34a6a10db5.png)
![image](https://user-images.githubusercontent.com/76146752/112165088-c74e0f80-8c31-11eb-9d27-7438a996d95b.png)

  - TP(True Positive) - 양성으로 예측했는데 맞은 개수
  - TN(True Negative) - 음성으로 예측했는데 맞은 개수
  - FP(False Positive) - 양성으로 예측했는데 틀린 개수 (음성을 양성으로 예측)
  - FN(False Negative) - 음성으로 예측했는데 틀린 개수 (양성을 음성으로 예측)
 
 #### 이진 분류 평가점수
  
  - Accuracy (정확도)
    - 전체 데이터 중에 맞게 예측한 것의 비율
    - 균형데이터일 때 주로 사용하면 괜찮음

  - Recall/Sensitivity(재현율/민감도)
    - 실제 Positive(양성)인 것 중에 Positive(양성)로 예측한 것의 비율
    - TPR(True Positive Rate) 이라고도 한다
    - ex) 스팸 메일 중 스팸메일로 예측한 비율, 금융 사기 데이터 중 사기로 예측한 비율

  - Precision(정밀도)
    - positive(양성)으로 예측한 것 중 실제 Positive(양성)인 비율
    - **PPV**(Positive Predictive Value)라고도 한다
    - ex) 스팸메일로 예측한 것 중에 스팸메일의 비율 금융 사기로 예측한 것 중 금융사기인 것의 비율

  - F1 점수
    - 정밀도와 재현율의 조화평균 점수
    - recall과 precision이 비슷할수록 높은 값을 가지게 된다 F1 score이 높다는 것은 recall과 precision이 한쪽으로 치우쳐 있지 않고 둘다  좋다고 판단할 수 있는 근거가 된다

 #### 기타
  - Specificity(특이도)
    - 실제 Negative(음성)인 것들 중 Negative(음성)으로 맞게 예측한 것의 비율
    - TNR(True Negative Rate)라고도 한다

  - Fall out(위양성률)
    - 실제 Negative(음성)인 것들 중 Positive(양성)으로 잘못 예측한 것의 비율 `1 - 특이도`
    - FPR (False Positive Rate) 라고도 한다
![image](https://user-images.githubusercontent.com/76146752/112170844-bc49ae00-8c36-11eb-91c5-222467132f50.png)ㅇ

#### 각 평가 지표 계산 함수
  - sklearn.metrics 모듈
  
  - confusion_matrix(y 실제값, x를 넣어 예측한 값)
    - 혼돈 행렬 반환
  - recall_score(y 실제값, x를 넣어 예측한 값)
    - Recall(재현율) 점수 반환 (Positive 중 Positive로 예측한 비율(TPR))

  - f1_score (y 실제값, x를 넣어 예측한 값)
    - F1 점수 반환 (recall과 precision의 조화 평균값)

  - classification_report(y 실제값, x를 넣어 예측한 값)
    - 클래스 별로 recall, precision, f1점수와 accuracy를 종합해서 보여준다

 #### Dummy 모델 혼동행렬
  - 
    ``` python
        from sklearn.metrics import confusion_matrix, plot_confusion_matrix, recall_score, precision_score, f1_score,accuracy_score, classificaition_report
        
        print('Train confusion matrix')
        print(confusion_matrix(y_train, pred_train))
        
        print('Test confusion matrix')
        print(confution_matrix(y_test, pred_test))
        
        from sklearn.dummy import DummyClassifier
        
        dummy_model = DummyClassifier(strategy = 'most_frequent') # 전략 : most_frequent(최빈값)
        dummy_model.fit(X_train, y_train)
        
        fig, ax = plt.subplots(1,1,figsize=(7,7))
        plot_confusion_matrix(dummy_model, # 학습시킨모델
                              X_train, # X
                              y_train, # y
                              display_labels = ['Neg', 'Pos'],
                              cmap = "Blues",
                              ax = ax )
         plt.show()
     ```
   ![image](https://user-images.githubusercontent.com/76146752/112174731-fe282380-8c39-11eb-970f-66d6ca2ac7db.png)
  
  #### dummy 모델 Accuracy, Recall, Precision, F1-Score
  
   - 
     ``` python
         
         print("Accuracy")
         accuracy_score(y_train, pred_train), accuracy_score(y_test, pred_test)
         
         print('Recall') # 9(양성) 중 9로 예측하여 맞은 것
         recall_score(y_train, pred_train), recall_score(y_test,pred_test)
         
         print('Precision') # 양성으로 예측한 것 중 실제로 양성 인것
         precision_score(y_train,pred_train), precision_test(y_test, pred_test)
         
         print('F1-Score')
         f1_score(y_train, pred_train), f1_score(y_test, pred_test)
         
         print('Classification Report')
         print(classification_report(y_train, pred_train), classification(y_test, pred_test))
     ```
     
  #### 재현율과 정밀도의 관계

  이진 분류의 경우 Precision(정밀도)가 중요한 경우와 Recall(재현율)이 중요한 업무가 있다
  
  - 재현율(recall)이 더 중요한 경우
    - 실제 Positive 데이터를 Negative로 잘못 판단하면 업무상 큰 영향이 있는 경우.sh
    - FN(False Negative)를 낮추는데 초점을 맞춘다
    - 암환자 판정 모델, 보험사기적발 모뎅

  - 정밀도가 더 중요한 경우
     - 실제 Negative 데이터를 Positive로 잘못 판단하면 업무상 큰 영향이 있는 경우
     - FP(False Positive)를 낮추는데 초점을 맞춘다
     - 스팸메일 판정

  #### 임계값(Threshold) 변경을 통한 재현율, 정밀도 변환
   - 임계값: 모델이 분류의 답을 결정할 때 기준값
   - 정밀도나 재현율을 특히 강조해야하는 상황일 경우 임계값 변경을 통해 평가 수치를 올릴 수 있다
   - 단 극단적으로 임계점을 올리거나 낮춰서 한쪽 점수만 높이면 안됨 (ex: 암환자 예측시 재현율을 너무 높이면 정밀도가 낮아져 걸핏하면 정상인을 암환자로 예측하게 된다)

  ##### 임계값 변경에 따른 정밀도와 재현율 변화관계
  - 임계값을 높이면 양성으로 예측하는 기준을 높여서 (엄격해짐) 음성으로 예측되는 샘픙이 많아진다. 그래서 정밀도는 높아지고 재현율은 낮아진다
  - 임계값을 낮추면 양성으로 예측하는 기준이 낮아져서 양성으로 예측되는 샘픙이 많아진다 그래서 재현율은 높아지고 정밀도는 낮아진다
    - **임계값을 낮추면 재현율은 올라가고 정밀도는 낮아진다**
    - **임계값을 높이면 재현율은 낮아지고 정밀도는 올라간다**
  - 임계값을 변화시켰을 때 재현율과 정밀도는 음의 상관관계를 가진다
  - 임계값을 변화시켰을때 재현율과 위양성율(Fall-Out/FPR)은 양의 상관관계를 가진다
 ![image](https://user-images.githubusercontent.com/76146752/112181048-6a595600-8c3f-11eb-93c9-05be62d048bf.png)

  ##### 임계값 변화에 따른 recall, precision 변화
   - 
      ``` python
          from sklearn.metrics import precision_recall_curve
          pred_proba = tree.predict_proba(X_test)[:,1]

          precisions, recalls, thresholds = precision_recall_curve(y_test, pred_proba)


          pd.DataFrame(dict(threshold = thresholds, recall = recalls[:-1], precision = precisions[:-1]))

          plt.figure(figsize=(7,7))
          plt.plot(thresholds, precisions[:-1])
          plt.plot(thresholds, recalls[:-1])
          plt.legend()
          plt.grid(True)
          plt.show()
       ```
  ##### Binarizer - 임계값 변경
   - Transformer로 양성 여부를 선택하는 임계값을 변경할 수 있다
  
  - 
    ``` python
        from sklearn.preprocessing import Binarizer
        example = [[-0.3,0.5,0.7,0.4,0.6]]
        bi = Binarizer(threshold=0.5) # 임계값 0.5
        bi.fit(example)
        bi.transform(example)
        
        # 머신러닝 모델에 적용
        pos_proba = tree.predict_proba(X_test)
        binarizer = Binarizer(threshold=0.01) # threshold의 값에 따라 결과가 바뀜(임계값이 커지면 recall은 작아지고 precision은 커짐)
        binarizer.fit(pos_proba) # tree모델을 통해 나왔던 확률을 한번더 binarizer에 넣어 0혹은 1로 나오도록 함
        predict = binarizer.transform(pos_proba)[:,1] # positive 들의 확률
        
        print(recall_score(y_test, predict), precision_score(y_test, predict)) # 임계값을 변경하면서 recall과 precision을 확인하여 비교해본다
    ```
    
 ##### PR Curve(Precision Recall Curve-정밀도 재현율 곡선)와 AP Score(Average Precision Score)
  - 0~1사이의 모든 임계값에 대하여 재현율(recall)과 정밀도(precision)의 변화를 이용한 평가 지표
  - X축에 재현율, Y축에 정밀도를 놓고 임계값이 0 -> 1 변화할 때 두 값의 변화를 선 그래프로 그린다
  - AP Score
    - PR Curve의 성능평가 지표를 하나의 점수(숫자)로 평가한 것
    - PR Curve의 선 아래 면적을 계산한 값으로 높을수록 성능이 우수하다
  - 
    ``` python
        from sklearn.metrics import precision_recall_curve, plot_precision_recall_curve, average_precision_score
        
        pos_proba = tree.predict(X_test)[:,1]
        
        precisions, recalls, thresholds = precision_recall_curve(y_test,pos_proba)
        
        thresholds = np.append(thresholds, 1)
        
        pd.DataFrame(dict(threshold=threshods, recall = recalls, precision= precisions))
        
        plt.figure(figsize=(7,7))
        plt.plot(recalls,precisions, marker='o')  #X: recall, Y:precision
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.grid(True)
        plt.show()
    ```
    ![image](https://user-images.githubusercontent.com/76146752/112298196-1a7b9d00-8cda-11eb-9ecd-045aa87af104.png)
  - 
    ``` python
        fig, ax = plt.subplot(1,1, figsize=(7,7))
        plot_precision_recall_curve(tree,
                                    X_test,
                                    y_test,
                                    ax=ax)
        plt.grid(True)
        plt.show()
        
        average_precision_score(y_test,pos_proba) #AP score(y, pos_예측확률)
    ```
 ![image](https://user-images.githubusercontent.com/76146752/112299409-1dc35880-8cdb-11eb-924e-087d4d41eeaf.png)

 ##### ROC curve(Receiver Operating Characteristic Curve)와 AUC(Area Under the Curve) score
  - FPR(False Positive Rate-위양성율) - Negative를 positive로 잘못 예측
  - TPR(True Positive Rate- 재현율/민감도) - Positive를 Positive로 예측
  - ROC 곡선
    - 2진 분류의 모델 성능 평가 지표 중 하나
    - 불균형 데이터셋을 평가할 때 사용
    - FPR을 X축, TPR을 Y축으로 놓고 임계값을 변경해서 FPR이 변할 때 TPR이 어떻게 변하는지 나타내는 곡선
  - AUC
    - ROC곡선 아래쪽 면적
    - 0~1 사이 실수로 나오며 클수록 좋다
    - **AUC 점수기준**
        - 1.0 ~ 0.9 : 아주 좋음
        - 0.9 ~ 0.8 : 좋음
        - 0.8 ~ 0.7 : 괜찮은 모델
        - 0.7 ~ 0.6 : 의미는 있으나 좋은 모델은 아님
        - 0.6 ~ 0.5 : 좋지 않은 모델
 ![image](https://user-images.githubusercontent.com/76146752/112302215-2cf7d580-8cde-11eb-87f1-3a0357006d9f.png)

  - 
    ``` python
        from sklearn.metrics import roc_curve, plot_roc_curve

        pos_proba_tree = tree.predict_proba(X_test)[:,1]
        pos_proba_rf = rf.predict_proba(X_test)[:,1]

        fpr_tree,tpr_tree, threshold_tree = roc_curve(y_test, pos_proba_tree) # y, pos_예측확률
        fpr_rf,tpr_rf, threshold_rf = roc_curve(y_test, pos_proba_rf) # y, pos_예측확률
        
        plt.figure(figsize=(7,7))
        ax = plt.gca() # get current axes
        plot_roc_curve(tree, X_test,y_test, ax=ax)
        plot_roc_curve(rf, X_test, y_test, ax=ax)
        plt.grid(True)
        plt.show()
    ```
    
    
    
    
#### ROC Curve - PR Curve
- ROC: 이진분류에서 양성클래스 탐지와 음성클래스 탐지의 중요도가 비슷할 때 사용(개고양이 분류)
- PR curve(Precision Recall 커브): 양성 클래스 탐지가 음성클래스 탐지의 중요도보다 높을 경우 사용(암환자 진단)
