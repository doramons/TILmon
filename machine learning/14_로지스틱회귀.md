## 로지스틱 회귀 (LogisticRegression)
  - 선형회귀 알고리즘을 이용한 이진 분류 모델
  - Sample이 특정 클래스에 속할 확률을 추정한다

 ### 확률 추정
  - 선형회귀처럼 입력특성(Feature)에 가중치 합을 계산한 값을 로지스틱함수를 적용해 확률을 계산한다
   `𝑝̂ =𝜎(𝐖𝑇⋅𝐱)`
  `𝜎() : 𝑙𝑜𝑔𝑖𝑠𝑡𝑖𝑐함수, 𝐖:가중치, 𝐱: 입력특성`

### 로지스틱 함수
  - 0과 1 사이의 실수를 반환한다
  - S자 형태의 결과를 내는 **시그모이드 함수(sigmoid function)** 이다
  `𝜎(𝑥)=1/1+𝐞−𝑥`
- 샘플 x가 양성에 속할 확률 
- `𝑦̂ ={0 𝑝̂ <0.5`
-    `{1 𝑝̂ ≧0.5`
-    
  ``` python
      import matplotlib.pyplot as plt
      
      xx = np.linspace(-10,10,100)
      sig = 1/ (1+np.exp(-xx))
      
      plt.figure(figsize=(15,5))
      
      plt.plot(xx,xx, color = 'g', label = 'linear')
      plt.plot(xx,sig, color='b', linewidth = 2, label = 'logistic')
      
      plt.plot([-10,10], [0,0], color = 'k', linestyle='-')
      plt.plot([-10,10], [0.5,0.5], color = 'r', linestyle=':', label='y=0.5')
      
      plt.xlabel('x')
      plt.legend(bbox_to_anchor=(1,1), fontsize=20)
      plt.xlim(-10,10)
      plt.ylim(-0.1,1.1)
      plt.grid(True)
      plt.show()
  ```
  ![image](https://user-images.githubusercontent.com/76146752/113330281-8359b980-9359-11eb-80f0-718e2c09ef61.png)

#### 손실 함수(Loss Function)
  - LogisticRegression의 전체 데이터 셋에 대한 손실함수는 아래공식과 같다
  - 로그 손실(log loss), Binary Cross Entropy(이진분류) 라고 한다
  - `𝐿(𝐖)=−1/𝑚*∑[𝑦𝑖log(𝑝̂)+(1−𝑦𝑖)log(1−𝑝̂)]`
  -   `𝑦 : 실제값, 𝑝̂ : 예측확률`
  -  y(실제값)이 1인 경우 𝑦𝑖log(𝑝̂)이 손실을 계산
  -  y가 0인 경우 (1−𝑦𝑖)log(1−𝑝̂)이 손실을 계산
  -  𝑝̂  (예측확률)이 클수록 반환값은 작아지고 작을수록 값이 커진다

#### 최적화
  - 위 손실을 가장 적게하는 W(가중치)를 찾는다
  - 로그 손실함수는 최소값을 찾는 정규방정식이 없기 때문에
    **LogisticRegression은 경사하강법을 이용해 최적화를 진행한다.
  - 로그 손실을 W로 미분하면 다음과 같다
    - 아래 도함수로 기울기를 구해 기울기가 0이 될 때까지 W(가중치)들을 update한다
    - `∂/∂𝑤 = 1 / 𝑚 ∑𝑖=1𝑚(𝜎(𝐖𝑇⋅𝐱𝑖)−𝐲𝑖)𝑥`

#### LogisticRegression 주요 하이퍼파라미터
  - penalty:과적합을 줄이기 위한 규제방식
    - l1, l2 (기본값), 'elasticnet', none
  - C: 규제강도(기본값1) - 작을수록 규제가 강하다
  - max_iter(기본값 100) : 경사하강법 반복횟수





