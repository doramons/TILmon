### Boosting
 부스팅이란 단순하고 약한 학습기들을 결합해서 보다 정확하고 강력한 학습기(Strong learner)를 만드는 방식
 정확도 낮은 모델이 학습을 한 뒤 두번째 모델은 그 모델의 예측 오류를 보완하고 이러한 과정이 반복되는 식으로 학습이 진행됨
  - gradient boosting
      - 처음 모델은 y를 예측 두번째부터는 앞 모델이 만든 오류를 예측하고 그것을 앞의 모델에 업데이트하면 오류를 줄일 수 있음
      - 그 오류를 update할 때 뺄까 더할까를 gradient descent방법을 사용(미분해서 나오는 값의 음수를 취해서 적용)
      - 학습률을 작게하면 update가 조금씩 되고 크게하면 많이 됨 (학습률을 크게할 경우 학습데이터에만 맞는 학습기가 될 수 있어 과대 적합우려가 있음
#### GradientBoosting
  - 개별 모델로 DecisionTree를 사용
  - depth가 깊지 않은 트리를 많이 연결하여 이전 트리의 오차를 보정해나가는 방식으로 실행
  - 오차를 보정할 때 경사하강법(Gradient descent)을 사용
  - 얕은 트리를 많이 연결하여 각각의 트리가 데이터의 일부에 대해 예측을 잘 수행하도록 하고 그런 트리들이 모여 전체 성능을 높이는 것이 기본 아이디어
  - 분류와 회귀 둘 다 지원함(GradientBoostingClassification,GradientBoostingRegressor)
  - 훈련시간이 많이 걸리고, 트리기반 모델의 특성상 희소한 고차원 데이터에서는 성능이 안좋은 단점이 있다

#### 주요 파라미터
  - Decision Tree의 가지치기 관련 매개변수
  - learning rate(학습률)
     - 이전 tree의 오차를 얼마나 강하게 보정할 것인지 제어하는 값
     - 값이 크면 보정을 강하게 하여 복잡한 모델을 만든다 학습데이터의 정확도는 올라가지만 너무 강하게 할 경우 과대적합이 날 수 있다
     - 값을 작게 잡으면 보정을 약하게 하여 모델의 복잡도를 줄인다 과대적합을 줄일 수 있지만 성능자체는 낮아질 수 있다
     - 기본값 : 0.1

  - n_estimators
    - tree의 개수 지정 , 많을수록 복잡한 모델이 된다
  - n_iter_no_change, validation_fraction
      - validation_fraction에 지정한 비율만큼 n_iter_no_change에 지정한 반복횟수동안 검증점수가 좋아지지 않으면 학습 조기종료
  - 보통 max_depth를 낮춰 개별 트리의 복잡도를 낮춘다 (5가 넘지 않게) 그리고 n_estimators를 가용시간, 메모리 한도에 맞춘 뒤 적절한 learning_rate를 찾는다
  - 
    ``` python
        from sklearn.datasets import load_breast_cancer
        from sklearn.model_selection import train_test_split
        from sklearn.ensemble import GradientBoostingClassifier
        from sklearn.metrics import accuracy_score
        
        data = load_breast_cancer()
        X,y = data['data'], data['target']
        X_train,X_test,y_train,y_test = train_test_split(X,y, stratify = y, random_state=1)
        
        gb = GradientBoostingClassifier(random_state=1, max_depth=2)
        
        gb.fit(X_train,y_train)
        
        pred_train= gb.predict(X_train)
        pred_test = gb.predict(X_test)
        
        accuracy_score(y_train,pred_train), accuracy_score(y_test,pred_test)
        
        # feature의 중요도
        import pandas as pd
        fi = gb.feature_importances_
        fi_s = pd.Series(fi, index=data['feature_names'])
        
    ```
#### GridSearchCV를 이용해 최적의 하이퍼파라미터 찾기
  - 
     ``` python
         from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

         param = {
          'n_estimators' : [100,200,300,400,500], # tree의 개수
          'learning_rate' : [0.001,0.005,0.01,0.05,0.1,0.5], # 학습률
          'max_depth' : range(1,5),
          'subsample' : [0.5,0.7,1], # 학습시킬 sample의 비율
         }

          gb = GridBoostingClassifier(random_state=1)

          gs = GridSearchCV(gb,
                            param_grid = param,
                            scoring = 'accuracy',
                            cv = 3,
                            n_jobs=-1)
           gs.fit(X_train,y_train)

           gs.best_params_

           result_df = pd.DataFrame(gs.cv_results_)
           result_df.sort_values('rank_test_score')

           pred_test = gs.predict(X_test)

           accuracy_score(y_test,pred_test)
       ```
     
  #### XGBoost(Extra Gradient Boost)
   - Gradient Boost 알고리즘을 기반으로 개선해서 나온 모델
   - 캐글 경진대회에서 상위에 입상한 데이터 과학자들이 사용한 것으로 알려져 유명해짐
   - Gradient Boost의 단점인 느린 수행시간을 해결하고 과적합을 제어할 수 있는 규제를 제공하여 성능을 높임
   - 두가지 개발 방법
      - scikit_learn 래퍼 XGBoost 모듈 사용
      - 파이썬 래퍼 XGBoost 모듈 사용
   - 설치
      - `pip install xgboost`
      - `conda install -y -c anaconda py-xgboost`

 ### Scikit-learn 래퍼 XGBoost
  - XGBoost를 Scikt-learn 프레임워크와 연동할 수 있도록 개발됨
  - Sikit-learn의 Estimator들과 동일한 패턴으로 코드를 작성할 수 있다
  - GridSearchCV나 Pipeline 등 Scikit-learn이 제공하는 다양한 유틸리티들을 사용할 수 있다
  - XGBClassifier :분류
  - XGBRegressor  : 회귀

#### 주요 매개변수
  - learning_rate : 학습률, 보통 0.01, 0.2 사이의 값 사용
  - n_estimators : week tree 개수
  - max_depth : 트리의 depth 지정
  - 
    ``` python
        xgb = XGBCLassifier(n_estimators=200,
                            learning_rate = 0.5,
                            max_depth = 2,
                            random_state = 1)
         xgb.fit(X_train,y_train)
         
         pred_train = xgb.predict(X_train)
         pred_test = xgb.predict(X_test)
         
         accuracy_score(y_train,pred_train), accuracy_score(y_test,pred_test)
         
         fi = xgb.feature_importances_
         fi
     ```
         









