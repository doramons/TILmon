## ì„ í˜•íšŒê·€ ê°œìš”
  ì„ í˜• íšŒê·€ëŠ” ì¢…ì†ë³€ìˆ˜ yì™€ í•œ ê°œ ì´ìƒì˜ ë…ë¦½ë³€ìˆ˜ Xì™€ì˜ ì„ í˜• ìƒê´€ê´€ê³„ë¥¼ ëª¨ë¸ë§í•˜ëŠ” íšŒê·€ë¶„ì„ ê¸°ë²•
  
 ### ì„ í˜•íšŒê·€ ëª¨ë¸
 - `ğ‘¦ğ‘–^=ğ‘¤1ğ‘¥ğ‘–1+ğ‘¤2ğ‘¥ğ‘–2...+ğ‘¤ğ‘ğ‘¥ğ‘–ğ‘+ğ‘`
 - `ğ‘¦ğ‘–^=ğ°ğ‘‡â‹…ğ— `
 - ğ‘¦ğ‘–^ : ì˜ˆì¸¡ê°’
 - ğ‘¥ : íŠ¹ì„±(feature-ì»¬ëŸ¼)
 - ğ‘¤ : ê°€ì¤‘ì¹˜(weight), íšŒê·€ê³„ìˆ˜(regression coefficient). íŠ¹ì„±ì´  ğ‘¦ğ‘–^  ì— ì–¼ë§ˆë‚˜ ì˜í–¥ì„ ì£¼ëŠ”ì§€ ì •ë„
 - ğ‘ : ì ˆí¸
 - ğ‘ : p ë²ˆì§¸ íŠ¹ì„±(feature)/pë²ˆì§¸ ê°€ì¤‘ì¹˜
 - ğ‘– : ië²ˆì§¸ ê´€ì¸¡ì¹˜(sample)

 ### G:
  X : ê¸°ì˜¨, y: ì›”ë“œì½˜ íŒë§¤ëŸ‰
  X1 - ê¸°ì˜¨, X2: ê´‘ê³  => ê° featureì˜ ì˜í–¥ë ¥ => ê°€ì¤‘ì¹˜ W1,W2,
  
  ax+b : ì„ í˜•ê´€ê³„ => ê°€ì„¤(í†µê³„) => MLì—ì„  ì•Œê³ ë¦¬ì¦˜
  a,b ë¥¼ ì°¾ì•„ì„œ ì˜ˆì¸¡í•œë‹¤
  
  
### ì†ì‹¤(loss)í•¨ìˆ˜/ ì˜¤ì°¨(error)í•¨ìˆ˜ / ë¹„ìš©(cost)í•¨ìˆ˜ / ëª©ì (objective)í•¨ìˆ˜
  - ëª¨ë¸ì´ ì¶œë ¥í•œ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œ ê°’ ì‚¬ì´ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
  - í‰ê°€ ì§€í‘œë¡œ ì‚¬ìš©ë˜ê¸°ë„ í•˜ê³  ëª¨ë¸ì„ ìµœì í™” í•˜ëŠ”ë° ì‚¬ìš©ëœë‹¤

### ìµœì í™”(Optimize)
  - ì†ì‹¤í•¨ìˆ˜ì˜ ê°’ì´ ìµœì†Œí™” ë˜ë„ë¡ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ê³¼ì •
  - ìµœì í™”ì˜ ë‘ê°€ì§€ ë°©ë²•
      - ì •ê·œ ë°©ì •ì‹
      - ê²½ì‚¬í•˜ê°•ë²•

### ì „ì²˜ë¦¬
  ì„ í˜•íšŒê·€ ëª¨ë¸ì‚¬ìš©ì‹œ ì „ì²˜ë¦¬
    - ë²”ì£¼í˜•: ì›í•« ì¸ì½”ë”©
    - Feature Scalingì„ í†µí•´ì„œ ê° ì»¬ëŸ¼ë“¤ì˜ ê°’ì˜ ë‹¨ìœ„ë¥¼ ë§ì¶°ì¤€ë‹¤
        - StandardScalerë¥¼ ìš©í•´ scalingí•˜ëŠ” ê²½ìš° ì„±ëŠ¥ì´ ë” ë†’ì•„ì§€ëŠ” ê²½í–¥ì´ ìˆìŒ

  - 
    ``` python
        # ì›í•«ì¸ì½”ë”©
        chas_df = pd.get_dummies(df['CHAS'])
        chas_df.columns = ['CHAS_0','CHAS_1']
        df2 = df.join([chas_df])
        df2 = df2.drop(columns='CHAS')
        
        y = df2['MEDV']
        X = df2.drop(columns='MEDV')
        
        np.random.seed(10)
        
        from sklearn.model_selection import train_test_split
        X_train,X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)
        
        train_columns = X_train.columns
        
        # feature scaling
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
    ```
### LinearRegression
  - ê°€ì¥ ê¸°ë³¸ì ì¸ ì„ í˜• íšŒê·€ ëª¨ë¸
  - 
    ``` python
        from sklearn.linear_model import LinearRegression
        
        lr = LinearRegression()
        lr.fit(X_train,y_train)
        
        lr.coef_ # í•™ìŠµí•œ ê°€ì¤‘ì¹˜
        lr.intercept_ # í•™ìŠµí•œ ì ˆí¸
        
        pred_train = lr.predict(X_train_scaled)
        pred_test = lr.predict(X_test_scaled)
        
        # í‰ê°€ì§€í‘œ ì¶œë ¥ í•¨ìˆ˜
        def print_metrics(y,y_pred,title=None):
          mse = np.round(mean_squared_error(y,y_pred),3)
          rmse = np.round(np.sqrt(mse),3)
          r2 = np.round(r2_score(y,y_pred),3)
          mae = np.round(mean_absolute_error(y,y_pred),3)
          if title:
            print(title)
           print(f'MSE : {mse}, RMSE: {rmse}, R2 : {r2}, MAE : {mae}')
        
        print_metrics(y_train,pred_train,title='LinearRegressor: Train')
        print_metrics(y_test,pred_test, title = 'LinearRegressor: Test')
        
        # ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ê°€ê²© plotting
        import matplotlib.pyplot as plt
        
        plt.figure(figsize=(15,5))
        plt.plot(range(len(y_test)),y_test, label = 'ì‹¤ì œê°’', marker = 'x')
        plt.plot(range(len(pred_test)),pred_test, label = 'ì˜ˆì¸¡ê°’', marker = 'o')
        plt.legend()
        plt.show()
        
    ```
    ![image](https://user-images.githubusercontent.com/76146752/113170735-cba5a880-9281-11eb-9b93-3ad99c568270.png)

 #### ê·œì œ (Regularization)
  - ì„ í˜• íšŒê·€ ëª¨ë¸ì—ì„œ ê³¼ì í•© ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê°€ì¤‘ì¹˜(íšŒê·€ê³„ìˆ˜)ì— íŒ¨ë„í‹° ê°’ì„ ì ìš©í•˜ëŠ” ê²ƒ
  - ì…ë ¥ë°ì´í„°ì˜ Featureë“¤ì´ ë„ˆë¬´ ë§ì€ ê²½ìš° ê³¼ì í•©ì´ ë°œìƒ
  
  - í•´ê²°
      - ë°ì´í„°ë¥¼ ë” ìˆ˜ì§‘
      - Feature selection
          - ë¶ˆí•„ìš”í•œ Feature ì œê±°
      - ê·œì œë¥¼ í†µí•´ Featureë“¤ì— ê³±í•´ì§€ëŠ” ê°€ì¤‘ì¹˜ê°€ ì»¤ì§€ì§€ ì•Šë„ë¡ ì œí•œ

  ##### Ridge Regression
   - ì†ì‹¤í•¨ìˆ˜ì— ê·œì œí•­ìœ¼ë¡œ L2 Normì„ ë”í•´ì¤€ë‹¤
   - ê°ë§ˆëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ì„ ì–¼ë§ˆë‚˜ ê·œì œí• ì§€ ì¡°ì ˆí•¨
      - ğ›¼=0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ê·œì œê°€ ì•½í•´ì§„ë‹¤ (0ì¼ ê²½ìš° ì„ í˜• íšŒê·€ì™€ ë™ì¼)
      - ğ›¼ê°€ ì»¤ì§ˆìˆ˜ë¡ ëª¨ë“  ê°€ì¤‘ì¹˜ê°€ ì‘ì•„ì ¸ ì…ë ¥ë°ì´í„°ì˜ Featureì¤‘ ì¤‘ìš”í•˜ì§€ ì•Šì€ Featureì˜ outputì— ëŒ€í•œ ì˜í–¥ë ¥ì´ ì‘ì•„ì§€ê²Œ ëœë‹¤
        
















