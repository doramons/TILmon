## 선형회귀 개요
  선형 회귀는 종속변수 y와 한 개 이상의 독립변수 X와의 선형 상관관계를 모델링하는 회귀분석 기법
  
 ### 선형회귀 모델
 - `𝑦𝑖^=𝑤1𝑥𝑖1+𝑤2𝑥𝑖2...+𝑤𝑝𝑥𝑖𝑝+𝑏`
 - `𝑦𝑖^=𝐰𝑇⋅𝐗 `
 - 𝑦𝑖^ : 예측값
 - 𝑥 : 특성(feature-컬럼)
 - 𝑤 : 가중치(weight), 회귀계수(regression coefficient). 특성이  𝑦𝑖^  에 얼마나 영향을 주는지 정도
 - 𝑏 : 절편
 - 𝑝 : p 번째 특성(feature)/p번째 가중치
 - 𝑖 : i번째 관측치(sample)

 ### G:
  X : 기온, y: 월드콘 판매량
  X1 - 기온, X2: 광고 => 각 feature의 영향력 => 가중치 W1,W2,
  
  ax+b : 선형관계 => 가설(통계) => ML에선 알고리즘
  a,b 를 찾아서 예측한다
  
  
### 손실(loss)함수/ 오차(error)함수 / 비용(cost)함수 / 목적(objective)함수
  - 모델이 출력한 예측값과 실제 값 사이의 차이를 계산하는 함수
  - 평가 지표로 사용되기도 하고 모델을 최적화 하는데 사용된다

### 최적화(Optimize)
  - 손실함수의 값이 최소화 되도록 모델을 학습하는 과정
  - 최적화의 두가지 방법
      - 정규 방정식
      - 경사하강법

### 전처리
  선형회귀 모델사용시 전처리
    - 범주형: 원핫 인코딩
    - Feature Scaling을 통해서 각 컬럼들의 값의 단위를 맞춰준다
        - StandardScaler를 용해 scaling하는 경우 성능이 더 높아지는 경향이 있음

  - 
    ``` python
        # 원핫인코딩
        chas_df = pd.get_dummies(df['CHAS'])
        chas_df.columns = ['CHAS_0','CHAS_1']
        df2 = df.join([chas_df])
        df2 = df2.drop(columns='CHAS')
        
        y = df2['MEDV']
        X = df2.drop(columns='MEDV')
        
        np.random.seed(10)
        
        from sklearn.model_selection import train_test_split
        X_train,X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)
        
        train_columns = X_train.columns
        
        # feature scaling
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
    ```
### LinearRegression
  - 가장 기본적인 선형 회귀 모델
  - 
    ``` python
        from sklearn.linear_model import LinearRegression
        
        lr = LinearRegression()
        lr.fit(X_train,y_train)
        
        lr.coef_ # 학습한 가중치
        lr.intercept_ # 학습한 절편
        
        pred_train = lr.predict(X_train_scaled)
        pred_test = lr.predict(X_test_scaled)
        
        # 평가지표 출력 함수
        def print_metrics(y,y_pred,title=None):
          mse = np.round(mean_squared_error(y,y_pred),3)
          rmse = np.round(np.sqrt(mse),3)
          r2 = np.round(r2_score(y,y_pred),3)
          mae = np.round(mean_absolute_error(y,y_pred),3)
          if title:
            print(title)
           print(f'MSE : {mse}, RMSE: {rmse}, R2 : {r2}, MAE : {mae}')
        
        print_metrics(y_train,pred_train,title='LinearRegressor: Train')
        print_metrics(y_test,pred_test, title = 'LinearRegressor: Test')
        
        # 실제 값과 예측가격 plotting
        import matplotlib.pyplot as plt
        
        plt.figure(figsize=(15,5))
        plt.plot(range(len(y_test)),y_test, label = '실제값', marker = 'x')
        plt.plot(range(len(pred_test)),pred_test, label = '예측값', marker = 'o')
        plt.legend()
        plt.show()
        
    ```
    ![image](https://user-images.githubusercontent.com/76146752/113170735-cba5a880-9281-11eb-9b93-3ad99c568270.png)

 #### 규제 (Regularization)
  - 선형 회귀 모델에서 과적합 문제를 해결하기 위해 가중치(회귀계수)에 패널티 값을 적용하는 것
  - 입력데이터의 Feature들이 너무 많은 경우 과적합이 발생
  
  - 해결
      - 데이터를 더 수집
      - Feature selection
          - 불필요한 Feature 제거
      - 규제를 통해 Feature들에 곱해지는 가중치가 커지지 않도록 제한

  ##### Ridge Regression
   - 손실함수에 규제항으로 L2 Norm을 더해준다
   - 감마는 하이퍼파라미터로 모델을 얼마나 규제할지 조절함
      - 𝛼=0에 가까울수록 규제가 약해진다 (0일 경우 선형 회귀와 동일)
      - 𝛼가 커질수록 모든 가중치가 작아져 입력데이터의 Feature중 중요하지 않은 Feature의 output에 대한 영향력이 작아지게 된다
        
















