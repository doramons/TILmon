## 선형회귀 개요
  선형 회귀는 종속변수 y와 한 개 이상의 독립변수 X와의 선형 상관관계를 모델링하는 회귀분석 기법
  
 ### 선형회귀 모델
 - `𝑦𝑖^=𝑤1𝑥𝑖1+𝑤2𝑥𝑖2...+𝑤𝑝𝑥𝑖𝑝+𝑏`
 - `𝑦𝑖^=𝐰𝑇⋅𝐗 `
 - 𝑦𝑖^ : 예측값
 - 𝑥 : 특성(feature-컬럼)
 - 𝑤 : 가중치(weight), 회귀계수(regression coefficient). 특성이  𝑦𝑖^  에 얼마나 영향을 주는지 정도
 - 𝑏 : 절편
 - 𝑝 : p 번째 특성(feature)/p번째 가중치
 - 𝑖 : i번째 관측치(sample)

  X : 기온, y: 월드콘 판매량
  X1 - 기온, X2: 광고 => 각 feature의 영향력 => 가중치 W1,W2,
  
  ax+b : 선형관계 => 가설(통계) => ML에선 알고리즘
  a,b 를 찾아서 예측한다
  
  
### 손실(loss)함수/ 오차(error)함수 / 비용(cost)함수 / 목적(objective)함수
  - 모델이 출력한 예측값과 실제 값 사이의 차이를 계산하는 함수
  - 평가 지표로 사용되기도 하고 모델을 최적화 하는데 사용된다

### 최적화(Optimize)
  - 손실함수의 값이 최소화 되도록 모델을 학습하는 과정
  - 최적화의 두가지 방법
      - 정규 방정식
      - 경사하강법

### 전처리
  선형회귀 모델사용시 전처리
    - 범주형: 원핫 인코딩
    - Feature Scaling을 통해서 각 컬럼들의 값의 단위를 맞춰준다
        - StandardScaler를 용해 scaling하는 경우 성능이 더 높아지는 경향이 있음

  - 
    ``` python
        # 원핫인코딩
        chas_df = pd.get_dummies(df['CHAS'])
        chas_df.columns = ['CHAS_0','CHAS_1']
        df2 = df.join([chas_df])
        df2 = df2.drop(columns='CHAS')
        
        y = df2['MEDV']
        X = df2.drop(columns='MEDV')
        
        np.random.seed(10)
        
        from sklearn.model_selection import train_test_split
        X_train,X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)
        
        train_columns = X_train.columns
        
        # feature scaling
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
    ```
### LinearRegression
  - 가장 기본적인 선형 회귀 모델
  - 
    ``` python
        from sklearn.linear_model import LinearRegression
        
        lr = LinearRegression()
        lr.fit(X_train,y_train)
        
        lr.coef_ # 학습한 가중치
        lr.intercept_ # 학습한 절편
        
        pred_train = lr.predict(X_train_scaled)
        pred_test = lr.predict(X_test_scaled)
        
        # 평가지표 출력 함수
        def print_metrics(y,y_pred,title=None):
          mse = np.round(mean_squared_error(y,y_pred),3)
          rmse = np.round(np.sqrt(mse),3)
          r2 = np.round(r2_score(y,y_pred),3)
          mae = np.round(mean_absolute_error(y,y_pred),3)
          if title:
            print(title)
           print(f'MSE : {mse}, RMSE: {rmse}, R2 : {r2}, MAE : {mae}')
        
        print_metrics(y_train,pred_train,title='LinearRegressor: Train')
        print_metrics(y_test,pred_test, title = 'LinearRegressor: Test')
        
        # 실제 값과 예측가격 plotting
        import matplotlib.pyplot as plt
        
        plt.figure(figsize=(15,5))
        plt.plot(range(len(y_test)),y_test, label = '실제값', marker = 'x')
        plt.plot(range(len(pred_test)),pred_test, label = '예측값', marker = 'o')
        plt.legend()
        plt.show()
        
    ```
    ![image](https://user-images.githubusercontent.com/76146752/113170735-cba5a880-9281-11eb-9b93-3ad99c568270.png)

 ### 다항회귀 (Polynomial Regression)
 
  - 단순한 직선형보다 복잡한 비선형 데이터셋을 학습하기 위한 방법
  - Feature들을 거듭제곱한 것과 Feature들을 곱한 새로운 특성들을 추가한 뒤 선형모델을 훈련시킨다
  - `PolynomialFeatures` Transformer를 사용
  - 
    ``` python
        import numpy as np
        import matplotlib.pyplot as plt
        import matplotlib as mpl
        mpl.rcParams['font.family'] = 'malgun gothic'
        mpl.rcParams['axes.unicode_minus'] = False
        
        m = 100
        X = 6*np.random.rand(m,1) - 3
        y = X**2 + X + 2 + np.random.normal(0,1, size=(m,1))
        X.shape, y.shape
        
        plt.figure(figsize=(7,6))
        plt.scatter(X,y)
        plt.show()
     ```
   ![image](https://user-images.githubusercontent.com/76146752/113301679-3ca89700-933a-11eb-9529-cd752ffcec4e.png)
- LinearRegression(선형회귀)
- 
  ``` python
      from sklearn.linear_model import LinearRegression
      
      lr.fit(X,y)
      lr.coef_, lr.intercept_
      
      from sklearn.metrics import mean_squared_error, r2_score
      pred = lr.predict(X)
      mean_squared_error(y, pred), r2_score(y,pred)
      
      X_new = np.linspace(-3,3,100).reshape(-1,1)
      pred_new = lr.predict(X_new)
      
      plt.figure(figsize=(7,6))
      
      plt.scatter(X,y, alpha = 0.7)
      plt.plot(X_new,pred_new, color = 'red')
      
      plt.grid(True)
      plt.show()
  ```
  ![image](https://user-images.githubusercontent.com/76146752/113302878-762dd200-933b-11eb-9adc-8b82f7fc74c0.png)

 - X의 Feature를 늘려서 다항식이 되도록 처리
 - 
   ``` python
       from sklearn.preprocessing import PolynomialFeatures
       
       # degree: 최고차항
       poly_f = PolynomialFeatures(degree=2, include_bias = False) # 상수항 추가여부
       X_poly = poly_f.fit_transform(X)
       
       poly_f.get_feature_names()
       
       lr2 = LinearRegression()
       lr2.fit(X_poly,y)
       
       lr2.coef_, lr2.intercept_
       
       pred2 = lr2.predict(X_poly)
       mean_squared_error(y,pred2), r2_score(y,pred2)
       
       X_new_poly = poly_f.transform(X_new)
       y_new2 = lr2.predict(X_new_poly)
       
       plt.figure(figsize=(7,6))
       plt.scatter(X,y)
       plt.plot(X_new,y_new2, color='red')
    ```
  ![image](https://user-images.githubusercontent.com/76146752/113311240-0f60e680-9344-11eb-8d35-753a1025acba.png)

      
 #### 규제 (Regularization)
  - 선형 회귀 모델에서 과적합 문제를 해결하기 위해 가중치(회귀계수)에 패널티 값을 적용하는 것
  - 입력데이터의 Feature들이 너무 많은 경우 과적합이 발생
      - Feature수에 비해 관측치 수가 적은 경우 모델이 복잡해지면서 과적합이 발생할 수 있음
  
  - 해결
      - 데이터를 더 수집
      - Feature selection
          - 불필요한 Feature 제거
      - 규제를 통해 Feature들에 곱해지는 가중치가 커지지 않도록 제한

  ##### Ridge Regression
   - 손실함수에 규제항으로 L2 Norm을 더해준다
   - 𝛼 는 하이퍼파라미터로 모델을 얼마나 규제할지 조절함
      - 𝛼 = 0에 가까울수록 규제가 약해진다 (0일 경우 선형 회귀와 동일)
      - 𝛼 가 커질수록 모든 가중치가 작아져 입력데이터의 Feature중 중요하지 않은 Feature의 output에 대한 영향력이 작아지게 된다
    - 
      ``` python
          from sklearn.linear_model import Ridge
          
          ridge_1 = Ridge() # alpha :  기본값 : 1
          ridge_1.fit(X_train_scaled, y_train)
          
          pred_train = ridge_1.predict(X_train_scaled)
          pred_test = ridge_1.predict(X_test_scaled)
          
          print_metrics(y_train,pred_train, title='Ridge alpha 1 : Train')
          print_metircs(y_test, pred_test, title = 'Ridge alpha 1 : Test')
      ```
      
 ##### Lasso(Least Absolute Shrinkage and Selection Operator) Regression
  - 손실함수에 규제항으로 L1 Norm 더한다  𝛼∑𝑛𝑖=1|𝑤𝑖| (가중치 절대값의 합 * 𝛼)
  - Lasso 회귀의 상대벅으로 덜 중요한 특성의 가중치를 0으로 만들어 자동으로 Feature Selection이 된다
          
  - 
   `
   손실함수(𝑤)=MSE(𝑤)+𝛼∑|𝑤𝑖|
   `

  -  
      ``` python
          from sklearn.linear_model import Lasso
          import matplotlib.pyplot as plt
          alpha_list = [0.1,0.5,1,10,100,200]

          lasso_coef_df = pd.DataFrame()

          plt.figure(figsize=(7,25)
          for idx, alpha in enumerate(alpha_list, start=1):
            lasso = Lasso(alpha=alpha)
            lasso.fit(X_train_scaled,y_train)

            w = pd.Series(lasso.coef_, index= X_train.columns).sort_values()
            lasso_coef_df[f'alpha {alpha}'] = w
            plt.subplot(6,1,idx)

            plt.bar(x=w.index, height=w)
            plt.xticks(rotation=45)
            plt.ylim(-20,20)
            plt.title(f'Lasso alpha{alpha}')
           plt.tight_layer()
           plt.show()
       ```
  ![image](https://user-images.githubusercontent.com/76146752/113322855-2c9bb200-9350-11eb-8354-b06206e6a0fa.png)


 ### 엘라스틱넷
  - 릿지와 라쏘를 절충한 모델
  - 규제항에 릿지, 라쏘 규제항을 더해서 추가한다
  - 혼합비율 r을 사용해 혼합정도를 조절
  - r = 0 이면 릿지와 같고 r = 1 이면 라쏘와 같음
  - 
       `손실함수(𝑤) = MSE(𝑤) + 𝑟𝛼∑𝑖=1𝑛|𝑤𝑖| + 1−𝑟2𝛼∑𝑖=1𝑛𝑤2𝑖`
  - 
    ``` python
        from sklearn.linear_model import ElasticNet
        
        elastic = ElasticNet(alpha = 0.1, l1_ratio=0.6)
        elastic.fit(X_train_scaled, y_train)
        
        pred_train = elastic.predict(X_train_scaled)
        pred_test = elastic.predict(X_test_scaled)
        
        print_metrics(y_train, pred_train, title='ElasticNet alpha 0.1: Train')
        print_metircs(y_test,pred_test, title = 'ElasticNet alpha 0.1 : Test')
    ```
    
 ### 정리
  - 일반적으로 선형회귀의 경우 어느정도 규제가 있는 경우가 성능이 좋다
  - 기본적으로 릿지를 사용한다
  - Target에 영향을 주는 Feature가 몇개 뿐일 경우 특성을 가중치를 0으로 만들어주는 라쏘를 사용한다
  - Feature수가 샘플 수보다 많거나 feature간에 연관성이 높을 때는 엘라스틱넷을 사용한다
        
        




