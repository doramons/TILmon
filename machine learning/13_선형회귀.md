## ì„ í˜•íšŒê·€ ê°œìš”
  ì„ í˜• íšŒê·€ëŠ” ì¢…ì†ë³€ìˆ˜ yì™€ í•œ ê°œ ì´ìƒì˜ ë…ë¦½ë³€ìˆ˜ Xì™€ì˜ ì„ í˜• ìƒê´€ê´€ê³„ë¥¼ ëª¨ë¸ë§í•˜ëŠ” íšŒê·€ë¶„ì„ ê¸°ë²•
  
 ### ì„ í˜•íšŒê·€ ëª¨ë¸
 - `ğ‘¦ğ‘–^=ğ‘¤1ğ‘¥ğ‘–1+ğ‘¤2ğ‘¥ğ‘–2...+ğ‘¤ğ‘ğ‘¥ğ‘–ğ‘+ğ‘`
 - `ğ‘¦ğ‘–^=ğ°ğ‘‡â‹…ğ— `
 - ğ‘¦ğ‘–^ : ì˜ˆì¸¡ê°’
 - ğ‘¥ : íŠ¹ì„±(feature-ì»¬ëŸ¼)
 - ğ‘¤ : ê°€ì¤‘ì¹˜(weight), íšŒê·€ê³„ìˆ˜(regression coefficient). íŠ¹ì„±ì´  ğ‘¦ğ‘–^  ì— ì–¼ë§ˆë‚˜ ì˜í–¥ì„ ì£¼ëŠ”ì§€ ì •ë„
 - ğ‘ : ì ˆí¸
 - ğ‘ : p ë²ˆì§¸ íŠ¹ì„±(feature)/pë²ˆì§¸ ê°€ì¤‘ì¹˜
 - ğ‘– : ië²ˆì§¸ ê´€ì¸¡ì¹˜(sample)

  X : ê¸°ì˜¨, y: ì›”ë“œì½˜ íŒë§¤ëŸ‰
  X1 - ê¸°ì˜¨, X2: ê´‘ê³  => ê° featureì˜ ì˜í–¥ë ¥ => ê°€ì¤‘ì¹˜ W1,W2,
  
  ax+b : ì„ í˜•ê´€ê³„ => ê°€ì„¤(í†µê³„) => MLì—ì„  ì•Œê³ ë¦¬ì¦˜
  a,b ë¥¼ ì°¾ì•„ì„œ ì˜ˆì¸¡í•œë‹¤
  
  
### ì†ì‹¤(loss)í•¨ìˆ˜/ ì˜¤ì°¨(error)í•¨ìˆ˜ / ë¹„ìš©(cost)í•¨ìˆ˜ / ëª©ì (objective)í•¨ìˆ˜
  - ëª¨ë¸ì´ ì¶œë ¥í•œ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œ ê°’ ì‚¬ì´ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
  - í‰ê°€ ì§€í‘œë¡œ ì‚¬ìš©ë˜ê¸°ë„ í•˜ê³  ëª¨ë¸ì„ ìµœì í™” í•˜ëŠ”ë° ì‚¬ìš©ëœë‹¤

### ìµœì í™”(Optimize)
  - ì†ì‹¤í•¨ìˆ˜ì˜ ê°’ì´ ìµœì†Œí™” ë˜ë„ë¡ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ê³¼ì •
  - ìµœì í™”ì˜ ë‘ê°€ì§€ ë°©ë²•
      - ì •ê·œ ë°©ì •ì‹
      - ê²½ì‚¬í•˜ê°•ë²•

### ì „ì²˜ë¦¬
  ì„ í˜•íšŒê·€ ëª¨ë¸ì‚¬ìš©ì‹œ ì „ì²˜ë¦¬
    - ë²”ì£¼í˜•: ì›í•« ì¸ì½”ë”©
    - Feature Scalingì„ í†µí•´ì„œ ê° ì»¬ëŸ¼ë“¤ì˜ ê°’ì˜ ë‹¨ìœ„ë¥¼ ë§ì¶°ì¤€ë‹¤
        - StandardScalerë¥¼ ìš©í•´ scalingí•˜ëŠ” ê²½ìš° ì„±ëŠ¥ì´ ë” ë†’ì•„ì§€ëŠ” ê²½í–¥ì´ ìˆìŒ

  - 
    ``` python
        # ì›í•«ì¸ì½”ë”©
        chas_df = pd.get_dummies(df['CHAS'])
        chas_df.columns = ['CHAS_0','CHAS_1']
        df2 = df.join([chas_df])
        df2 = df2.drop(columns='CHAS')
        
        y = df2['MEDV']
        X = df2.drop(columns='MEDV')
        
        np.random.seed(10)
        
        from sklearn.model_selection import train_test_split
        X_train,X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)
        
        train_columns = X_train.columns
        
        # feature scaling
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
    ```
### LinearRegression
  - ê°€ì¥ ê¸°ë³¸ì ì¸ ì„ í˜• íšŒê·€ ëª¨ë¸
  - 
    ``` python
        from sklearn.linear_model import LinearRegression
        
        lr = LinearRegression()
        lr.fit(X_train,y_train)
        
        lr.coef_ # í•™ìŠµí•œ ê°€ì¤‘ì¹˜
        lr.intercept_ # í•™ìŠµí•œ ì ˆí¸
        
        pred_train = lr.predict(X_train_scaled)
        pred_test = lr.predict(X_test_scaled)
        
        # í‰ê°€ì§€í‘œ ì¶œë ¥ í•¨ìˆ˜
        def print_metrics(y,y_pred,title=None):
          mse = np.round(mean_squared_error(y,y_pred),3)
          rmse = np.round(np.sqrt(mse),3)
          r2 = np.round(r2_score(y,y_pred),3)
          mae = np.round(mean_absolute_error(y,y_pred),3)
          if title:
            print(title)
           print(f'MSE : {mse}, RMSE: {rmse}, R2 : {r2}, MAE : {mae}')
        
        print_metrics(y_train,pred_train,title='LinearRegressor: Train')
        print_metrics(y_test,pred_test, title = 'LinearRegressor: Test')
        
        # ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ê°€ê²© plotting
        import matplotlib.pyplot as plt
        
        plt.figure(figsize=(15,5))
        plt.plot(range(len(y_test)),y_test, label = 'ì‹¤ì œê°’', marker = 'x')
        plt.plot(range(len(pred_test)),pred_test, label = 'ì˜ˆì¸¡ê°’', marker = 'o')
        plt.legend()
        plt.show()
        
    ```
    ![image](https://user-images.githubusercontent.com/76146752/113170735-cba5a880-9281-11eb-9b93-3ad99c568270.png)

 ### ë‹¤í•­íšŒê·€ (Polynomial Regression)
 
  - ë‹¨ìˆœí•œ ì§ì„ í˜•ë³´ë‹¤ ë³µì¡í•œ ë¹„ì„ í˜• ë°ì´í„°ì…‹ì„ í•™ìŠµí•˜ê¸° ìœ„í•œ ë°©ë²•
  - Featureë“¤ì„ ê±°ë“­ì œê³±í•œ ê²ƒê³¼ Featureë“¤ì„ ê³±í•œ ìƒˆë¡œìš´ íŠ¹ì„±ë“¤ì„ ì¶”ê°€í•œ ë’¤ ì„ í˜•ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¨ë‹¤
  - `PolynomialFeatures` Transformerë¥¼ ì‚¬ìš©
  - 
    ``` python
        import numpy as np
        import matplotlib.pyplot as plt
        import matplotlib as mpl
        mpl.rcParams['font.family'] = 'malgun gothic'
        mpl.rcParams['axes.unicode_minus'] = False
        
        m = 100
        X = 6*np.random.rand(m,1) - 3
        y = X**2 + X + 2 + np.random.normal(0,1, size=(m,1))
        X.shape, y.shape
        
        plt.figure(figsize=(7,6))
        plt.scatter(X,y)
        plt.show()
     ```
   ![image](https://user-images.githubusercontent.com/76146752/113301679-3ca89700-933a-11eb-9529-cd752ffcec4e.png)
- LinearRegression(ì„ í˜•íšŒê·€)
- 
  ``` python
      from sklearn.linear_model import LinearRegression
      
      lr.fit(X,y)
      lr.coef_, lr.intercept_
      
      from sklearn.metrics import mean_squared_error, r2_score
      pred = lr.predict(X)
      mean_squared_error(y, pred), r2_score(y,pred)
      
      X_new = np.linspace(-3,3,100).reshape(-1,1)
      pred_new = lr.predict(X_new)
      
      plt.figure(figsize=(7,6))
      
      plt.scatter(X,y, alpha = 0.7)
      plt.plot(X_new,pred_new, color = 'red')
      
      plt.grid(True)
      plt.show()
  ```
  ![image](https://user-images.githubusercontent.com/76146752/113302878-762dd200-933b-11eb-9adc-8b82f7fc74c0.png)

 - Xì˜ Featureë¥¼ ëŠ˜ë ¤ì„œ ë‹¤í•­ì‹ì´ ë˜ë„ë¡ ì²˜ë¦¬
 - 
   ``` python
       from sklearn.preprocessing import PolynomialFeatures
       
       # degree: ìµœê³ ì°¨í•­
       poly_f = PolynomialFeatures(degree=2, include_bias = False) # ìƒìˆ˜í•­ ì¶”ê°€ì—¬ë¶€
       X_poly = poly_f.fit_transform(X)
       
       poly_f.get_feature_names()
       
       lr2 = LinearRegression()
       lr2.fit(X_poly,y)
       
       lr2.coef_, lr2.intercept_
       
       pred2 = lr2.predict(X_poly)
       mean_squared_error(y,pred2), r2_score(y,pred2)
       
       X_new_poly = poly_f.transform(X_new)
       y_new2 = lr2.predict(X_new_poly)
       
       plt.figure(figsize=(7,6))
       plt.scatter(X,y)
       plt.plot(X_new,y_new2, color='red')
    ```
  ![image](https://user-images.githubusercontent.com/76146752/113311240-0f60e680-9344-11eb-8d35-753a1025acba.png)

      
 #### ê·œì œ (Regularization)
  - ì„ í˜• íšŒê·€ ëª¨ë¸ì—ì„œ ê³¼ì í•© ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê°€ì¤‘ì¹˜(íšŒê·€ê³„ìˆ˜)ì— íŒ¨ë„í‹° ê°’ì„ ì ìš©í•˜ëŠ” ê²ƒ
  - ì…ë ¥ë°ì´í„°ì˜ Featureë“¤ì´ ë„ˆë¬´ ë§ì€ ê²½ìš° ê³¼ì í•©ì´ ë°œìƒ
      - Featureìˆ˜ì— ë¹„í•´ ê´€ì¸¡ì¹˜ ìˆ˜ê°€ ì ì€ ê²½ìš° ëª¨ë¸ì´ ë³µì¡í•´ì§€ë©´ì„œ ê³¼ì í•©ì´ ë°œìƒí•  ìˆ˜ ìˆìŒ
  
  - í•´ê²°
      - ë°ì´í„°ë¥¼ ë” ìˆ˜ì§‘
      - Feature selection
          - ë¶ˆí•„ìš”í•œ Feature ì œê±°
      - ê·œì œë¥¼ í†µí•´ Featureë“¤ì— ê³±í•´ì§€ëŠ” ê°€ì¤‘ì¹˜ê°€ ì»¤ì§€ì§€ ì•Šë„ë¡ ì œí•œ

  ##### Ridge Regression
   - ì†ì‹¤í•¨ìˆ˜ì— ê·œì œí•­ìœ¼ë¡œ L2 Normì„ ë”í•´ì¤€ë‹¤
   - ğ›¼ ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ì„ ì–¼ë§ˆë‚˜ ê·œì œí• ì§€ ì¡°ì ˆí•¨
      - ğ›¼ = 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ê·œì œê°€ ì•½í•´ì§„ë‹¤ (0ì¼ ê²½ìš° ì„ í˜• íšŒê·€ì™€ ë™ì¼)
      - ğ›¼ ê°€ ì»¤ì§ˆìˆ˜ë¡ ëª¨ë“  ê°€ì¤‘ì¹˜ê°€ ì‘ì•„ì ¸ ì…ë ¥ë°ì´í„°ì˜ Featureì¤‘ ì¤‘ìš”í•˜ì§€ ì•Šì€ Featureì˜ outputì— ëŒ€í•œ ì˜í–¥ë ¥ì´ ì‘ì•„ì§€ê²Œ ëœë‹¤
    - 
      ``` python
          from sklearn.linear_model import Ridge
          
          ridge_1 = Ridge() # alpha :  ê¸°ë³¸ê°’ : 1
          ridge_1.fit(X_train_scaled, y_train)
          
          pred_train = ridge_1.predict(X_train_scaled)
          pred_test = ridge_1.predict(X_test_scaled)
          
          print_metrics(y_train,pred_train, title='Ridge alpha 1 : Train')
          print_metircs(y_test, pred_test, title = 'Ridge alpha 1 : Test')
      ```
      
 ##### Lasso(Least Absolute Shrinkage and Selection Operator) Regression
  - ì†ì‹¤í•¨ìˆ˜ì— ê·œì œí•­ìœ¼ë¡œ L1 Norm ë”í•œë‹¤  ğ›¼âˆ‘ğ‘›ğ‘–=1|ğ‘¤ğ‘–| (ê°€ì¤‘ì¹˜ ì ˆëŒ€ê°’ì˜ í•© * ğ›¼)
  - Lasso íšŒê·€ì˜ ìƒëŒ€ë²…ìœ¼ë¡œ ëœ ì¤‘ìš”í•œ íŠ¹ì„±ì˜ ê°€ì¤‘ì¹˜ë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ì–´ ìë™ìœ¼ë¡œ Feature Selectionì´ ëœë‹¤
          
  - 
   `
   ì†ì‹¤í•¨ìˆ˜(ğ‘¤)=MSE(ğ‘¤)+ğ›¼âˆ‘|ğ‘¤ğ‘–|
   `

  -  
      ``` python
          from sklearn.linear_model import Lasso
          import matplotlib.pyplot as plt
          alpha_list = [0.1,0.5,1,10,100,200]

          lasso_coef_df = pd.DataFrame()

          plt.figure(figsize=(7,25)
          for idx, alpha in enumerate(alpha_list, start=1):
            lasso = Lasso(alpha=alpha)
            lasso.fit(X_train_scaled,y_train)

            w = pd.Series(lasso.coef_, index= X_train.columns).sort_values()
            lasso_coef_df[f'alpha {alpha}'] = w
            plt.subplot(6,1,idx)

            plt.bar(x=w.index, height=w)
            plt.xticks(rotation=45)
            plt.ylim(-20,20)
            plt.title(f'Lasso alpha{alpha}')
           plt.tight_layer()
           plt.show()
       ```
  ![image](https://user-images.githubusercontent.com/76146752/113322855-2c9bb200-9350-11eb-8354-b06206e6a0fa.png)


 ### ì—˜ë¼ìŠ¤í‹±ë„·
  - ë¦¿ì§€ì™€ ë¼ì˜ë¥¼ ì ˆì¶©í•œ ëª¨ë¸
  - ê·œì œí•­ì— ë¦¿ì§€, ë¼ì˜ ê·œì œí•­ì„ ë”í•´ì„œ ì¶”ê°€í•œë‹¤
  - í˜¼í•©ë¹„ìœ¨ rì„ ì‚¬ìš©í•´ í˜¼í•©ì •ë„ë¥¼ ì¡°ì ˆ
  - r = 0 ì´ë©´ ë¦¿ì§€ì™€ ê°™ê³  r = 1 ì´ë©´ ë¼ì˜ì™€ ê°™ìŒ
  - 
       `ì†ì‹¤í•¨ìˆ˜(ğ‘¤) = MSE(ğ‘¤) + ğ‘Ÿğ›¼âˆ‘ğ‘–=1ğ‘›|ğ‘¤ğ‘–| + 1âˆ’ğ‘Ÿ2ğ›¼âˆ‘ğ‘–=1ğ‘›ğ‘¤2ğ‘–`
  - 
    ``` python
        from sklearn.linear_model import ElasticNet
        
        elastic = ElasticNet(alpha = 0.1, l1_ratio=0.6)
        elastic.fit(X_train_scaled, y_train)
        
        pred_train = elastic.predict(X_train_scaled)
        pred_test = elastic.predict(X_test_scaled)
        
        print_metrics(y_train, pred_train, title='ElasticNet alpha 0.1: Train')
        print_metircs(y_test,pred_test, title = 'ElasticNet alpha 0.1 : Test')
    ```
    
 ### ì •ë¦¬
  - ì¼ë°˜ì ìœ¼ë¡œ ì„ í˜•íšŒê·€ì˜ ê²½ìš° ì–´ëŠì •ë„ ê·œì œê°€ ìˆëŠ” ê²½ìš°ê°€ ì„±ëŠ¥ì´ ì¢‹ë‹¤
  - ê¸°ë³¸ì ìœ¼ë¡œ ë¦¿ì§€ë¥¼ ì‚¬ìš©í•œë‹¤
  - Targetì— ì˜í–¥ì„ ì£¼ëŠ” Featureê°€ ëª‡ê°œ ë¿ì¼ ê²½ìš° íŠ¹ì„±ì„ ê°€ì¤‘ì¹˜ë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ëŠ” ë¼ì˜ë¥¼ ì‚¬ìš©í•œë‹¤
  - Featureìˆ˜ê°€ ìƒ˜í”Œ ìˆ˜ë³´ë‹¤ ë§ê±°ë‚˜ featureê°„ì— ì—°ê´€ì„±ì´ ë†’ì„ ë•ŒëŠ” ì—˜ë¼ìŠ¤í‹±ë„·ì„ ì‚¬ìš©í•œë‹¤
        
        




