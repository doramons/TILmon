### 최적화 (Optimize)
  - 모델이 예측한 결과와 실제 값의 차이를 줄이기 위해서 모델을 수정해야하는 작업을 최적화라고 한다
  - 모델의 예측값과 실제 값의 차이를 계산하는 함수를 만들고 그 값이 최소가 되는 지점을 찾는 작업을 한다

### 최적화 문제
  - 함수 f(x) 의 값을 최소화(또는 최대화) 하는 변수 x의 값을 찾는 것
  - `𝑥𝑖 = argmin𝑓(𝑥)`
  - 
    ``` python
        import numpy as np
        import matplotlib.pyplot as plt
        
        def loss_func(x):
          return (x-1)**2 + 2
        
        plt.figure(figsize=(10,10))
        
        xx = np.linspace(-3,4,100)
        plt.plot(xx, loss_func(xx))
        plt.plot(1,2, 'ro', markersize=10)
        plt.ylim(0,10)
        plt.xlim(-3,5)
        plt.xlabel('x')
        plt.ylabel('$f(x)$')
        plt.grid(True)
        plt.show()
     ```

![image](https://user-images.githubusercontent.com/76146752/113404042-864cbc80-93e2-11eb-836d-514097ef4183.png)


#### 목적함수(Object Function), 손실함수(Loss Function), 비용함수(Cost Function), 오차함수(Error Function)
  - 모델의 예측한 값과 실제 값 사이의 오차를 정의하는 함수
  - 이 함수를 최소화하는 값을 찾는 것이 최적화의 목적

### 최적화 문제를 해결하는 방법
  - 최소값을 찾는 함수(공식)을 찾는다
      - 공식을 찾을 수 없는 경우가 있다
      - feature와 sample 수가 많아질수록 계산량이 급증한다
  - 경사하강법 (Gradient Descent)
      - 값을 조금씩 조금씩 조정해나가면서 최소값을 찾는다



### 경사하강법 (Gradient Descent)
  - 다양한 종류의 문제에서 최적의 해법을 찾을 수 있는 일반적인 최적화 알고리즘
  - 비용함수를 최소화하는 파라미터를 찾기 위해 반복해서 조정해 나간다
      - 파라미터 벡터 𝑊에 대해 손실함수의 현재 gradient(경사,기울기)를 계산한다
      - gradient가 감소하는 방향으로 벡터 𝑊를 조정한다
      - gradient가 0이 될때까지 반복한다
![image](https://user-images.githubusercontent.com/76146752/113407298-f742a300-93e7-11eb-8560-2ebd97640939.png)

#### 파라미터 조정

`𝑊𝑛𝑒𝑤 = 𝑊−𝛼*∂/∂𝑊*𝑐𝑜𝑠𝑡(𝑊)`
 
`𝑊 : 파라미터, 𝛼 :학습률`

  - 학습률 (Learning rate)
    - 기울기에 따라 이동할 step의 크기. 경사하강법 알고리즘에서 지정해야하는 하이퍼파라미터이다
    - 학습률을 너무 작게 잡으면 최소값에 수렴하기 위해 많은 반복을 진행해야해서 시간이 오래걸림
    - 학습률을 너무 크게 잡으면 왔다 갔다 하다가 오히려 더 큰 값으로 발산하여 최소값에 수렴하지 못하게 된다
    - 
      ``` python
          import numpy as np
          def f(x):
            return (x-1)**2 +2
          # 함수 f의 도함수
          def fd(x):
            return 2*(x-1)
            
          X = np.linspace(-3,4,100)
          
          plt.figure(figsize=(10,10))
          plt.plot(X, f(X), 'k-')
          
          learning_rate = 0.4
          # learning_rate는 보통 0.1~0.01 사이값에서 시작하도록 설정하곤 함
          
          # w : 조정할 가중치
          w = 0
          plt.plot(w, f(w), 'go', markersize=10)
          plt.text(w+0.1, f(w)+0.1. '1차시도')
          plt.plot(X, fd(w)*(X) + f(w), 'g--')
          print('1차시도: w={:.2f}, 기울기 = {:.2f}'.format(w, fd(w)))
          
          w = w - learning_rate*fd(w)
          plt.plot(w, f(w), 'ro', markersize=10)
          plt.text(w+0.1,f(w)+0.1, '2차시도')
          plt.plot(X, fd(w)*(X-w) +f(w), 'r--')
          print('2차시도: w{:.2f}, 기울기 = {:.2f}'.format(w, fd(w)))
          
          w = w - learning_rate*fd(w)
          plt.plot(w, f(w), 'ro', markersize=10)
          plt.text(w+0.1, f(w) + 0.1, '3차시도')
          plt.plot(X, fd(w)*(X-w) + f(w), 'b--')
          print('3차시도: w={:.2f}, 기울기 = {:.2f}'.format(w, fd(w)))
          
          w = w - learning_rate*fd(w)
          plt.plot(w, f(w), 'mo', markersize=10)
          plt.text(w+0.1, f(w)+0.1, '4차시도')
          plt.plot(X, fd(w)*(X-w) + f(w), 'm--')
          print('4차시도: w={:.2f}, 기울기 = {:.2f}'.format(w, fd(w)))
          
          plt.ylim(0,15)
          plt.xlim(-4,10)
          plt.xlabel('W')
          plt.ylabel('Cost')
          plt.grid(True)
          plt.show()
       ```
    ![image](https://user-images.githubusercontent.com/76146752/113424057-7b0d8700-940a-11eb-9886-a92cf1ba6fa2.png)
  - 
    ``` pyhton
        w = 0
        cnt = 0
        while True:
            if fd(w) == 0:
                break
            w = w-learning_rate*fd(w)
            cnt += 1
     ```





