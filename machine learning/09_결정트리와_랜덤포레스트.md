## 의사결정나무(Decision Tree)
![image](https://user-images.githubusercontent.com/76146752/112937121-116b4f80-9162-11eb-9f98-ead0ab7e187f.png)

### 개요
 - 데이터를 잘 분류할 수 있는 질문을 던져가며 대상을 좁혀가는 '스무고개'와 비슷한 형식의 알고리즘
 - 분기해 나가는 구조가 Tree 구조와 같기 때문에 Decision Tree라고 함
    - 불순도를 최대한 감소하는 방향으로 조건을 만들어서 학습을 진행한다
    - 하위 노드는 yes/no 두개로 분기됨
 - 머신러닝 모델들 중 몇 안되는 white box 모델로 결과에 대한 해석이 가능하다
 - 과대적합(Overfitting)이 발생하기 쉽다
 - 앙상블기반 알고리즘인 랜덤 포레스트와 많은 부스팅(boosting)기반 앙상블 모델들의 기반 알고리즘으로 사용됨
 > 순도(purity)/불순도(impurity)
 >  - 서로 다른 종류의 값들이 섞여 있는 비율
 >  - 한 종류(class)의 값이 많을수록 순도가 높고 분순도는 낮다

### 용어
  - Root Node : 시작 node
  - Decisionn Node (Intermediate Node) : 중간 node
  - Leaf Node(Terminal Node) : 마지막단계의 노드로 최종결과를 가진다

### 과대적합(Overfitting) 문제
  - 의사결정나무는 모든 데이터셋이 모두 잘 분류되어 불순도가 0이 될때까지 분기해 나간다 -> 모델중에 과대적합이 잘 일어나는 모델임
  - Root에서부터 하위노드가 많이 만들어질수록 모델이 복잡해져 과대적하빙 발생할 수 있ㄷ음
  - 과대적합을 막기 위해서 적당한 시점에 하위노드가 생성되지 않도록 해야함 -> 가지치기 (Pruning)
 
 #### 하이퍼파라미터
  - 가지치기 관련 하이퍼파라미터
    - max_depth : 최대깊이
    - max_leaf_nodes : 생성될 최대 Leaf Node 개수 제함
    - min_samples_leaf : 가지를 칠 최소 sample 수 - sample수가 지정한 값보다 작으면 불순도와 상관없이 가치를 치지 않음
  - criterion
    - 불순도 계산 방식을 하이퍼파라미터
    - gini (기본값)
    - entropy ( 불순도를 측정하는 지표로서 정보량의 기댓값)
 
 #### Feature(컬럼) 중요도 조회
  - feature_importances_ 속성
    - 모델을 만들 때 각 Feature의 중요도를 반환
    - input data에서 중요한 feature을 찾기 위해 decision tree를 이용하기도 함

  #### wine color
   - features
    - 와인 화학성분들
        - fixed acidity : 고정 산도
        - volatile acidity : 휘발성 산도
        - citric acid : 시트르산
        - residual sugar : 잔류 당분
        - chlorides : 염화물
        - free sulfur dioxide : 자유 이산화황
        - total sulfur dioxide : 총 이산화황
        - density : 밀도
        - pH : 수소 이온 농도
        - sulphates : 황산염
        - alcohol : 알콜
      - quality : 와인등급 (A>B>C)
   - target : color
      - 0 : white, 1: red
  - Feature_importances_ 조회
  - 
    ``` python
        wine = pd.read_csv('data/wine.csv')
        X = wine.drop('color',axis=1)
        y = wine['color']
        
        le = LabelEncoder()
        wine['quality'] = le.fit_transform(wine['quality']) # quality 열이 문자열 이므로 라벨링 해줌(전처리)
        
        X_train,X_test,y_train,y_test = train_test_split(X,y,stratify=y, random_state=1)
        
        tree = DecisionTreeClassifier()
        params = {'max_depth': range(2,15),
                  'min_samples_leaf': [100,500,1000,2000],
                  'max_leaf_nodes' : [5,10,15,20,25,30]}
        
        gs = GridSearchCV(tree,
                          param_grid = params,
                          scoring= 'accuracy',
                          cv = 3,
                          n_jobs = -1)
        gs.fit(X_train,y_train)
        
        gs.best_params_
        
        best_tree = gs.best_estimator_
        fi = best_tree.feature_importances_
        
        fi_s = pd.Series(fi,index = X_train.columns)
        
        fi_s.sort_values(ascending=False)
        # Decision Tree는 유의미한 feature를 선택하는데 도움을 주기도 함
        
 ## 앙상블(Ensemble) 기법
  - 하나의 모델만을 학습시켜 사용하지 않고 여러 모델을 학습시켜 결합하는 방식으로 문제를 해결하는 방식
  - 개별로 학습함 여러 보델을 여러가지 방법으로 조합하여 과적합을 막고 일반화 성능을 향상
  - 개별 모델의 성능이 확보되지 않을 때 성능향상에 도움될 수 있다

  ### 앙상블 종류
   1. 투표방식
    - 여러개의 추정기(Estimator)가 낸 결과들을 투표를 통해 최종 결과를 내는 방식
    - 종류
      1. Bagging - 같은 유형의 알고리즘들을 조합하되 각각 학습하는 데이터를 다르게 한다
      2. Voting - 서로 다른 종류의 알고리즘들을 결합한다
   2. 부스팅 (Boosting)
    - 약한 학습기(Weak Learner)들을 결합해서 보다 정확하고 강력한 학습기(Strong Learner)를 만든다
    - 각 약한 학습기들은 순서대로 일을 하며 뒤의 학습기들은 앞의 학습기의 오류를 토대로 이를 보완하는 방식으로 학습한다

  ### Random Forest(랜덤포레스트)
  
   - Bagging 방식의 앙상블 모델
   - DecisionTree를 기반으로 한다
   ![image](https://user-images.githubusercontent.com/76146752/112974159-1db8d200-918d-11eb-9c68-e2ba17ab4e1c.png)

   - 다수의 결정트리를 사용해서 성능을 올린 앙상블 알고리즘의 하나
      - 학습 데이터를 샘플링해서 다수의 결정트리를 생성하고 이를 기반으로 다수결로 결과 결정
      - 다수의 결정트리를 만드는데서 랜덤포레스트라고 부른다
   - 처리속도가 빠르며 분류성능도 높은 모델로 알려져 있다
   - 랜덤포레스트의 절차
      - 결정트리의 개수를 하이퍼파라미터로 받는다
      - 랜덤 포레스트를 구성하는 모든 결정 트리가 서로 다르도록 만든다
          - 각 트리는 **부트스트랩 샘플링**(중복을 허용하면서 랜덤하게 샘플링)으로 데이터셋을 준비
          - 각 트리는 전체 Feature중 일부 Feature를 랜덤으로 가지게 됨
      - 각 트리별로 예측결과를 내고 분류의 경우 그 예측을 모아 다수결 투표로 클래스 결과를 낸다
      - 회귀의 경우에는 예측 결과의 평균을 낸다

   - 주요 하이퍼파라미터
      - n_estimators
          - tree의 개수
          - 시간과 메모리가 허용하는 범위에서 클수록 좋음
      - max_features
          - 각 트리에서 선택할 feature의 개수
          - 클수록 각 트리간의 feature 차이가 없어지고 작을수록 차이가 많이남
      - max_depth, min_samples_leaf, max_leaf_nodes.. ( DecisionTreeClassifier 의 파라미터)
        
        
        
