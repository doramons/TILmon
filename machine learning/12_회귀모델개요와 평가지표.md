## 회귀(Regression)
  지도학습(Supervised Learning)으로 예측할 Target이 연속형(continuous) 데이터(float)인 경우
  
### 회귀의 주요 평가 지표
  예측값과 실제 값 간의 차이(오차)를 구한다
  
  - MSE (Mean Squared Error)
  
  - RMSE (Root Mean Squared Error)
  
  - R^2 (R square, 결정계수)
    - 평균으로 예측했을 때 오차(총 오차) 보다 모델을 사용했을 때 얼마만큼 더 좋은 성능을 내는지를 비율로 나타낸 값
    - 1에 가까울 수록 좋은 모델
    - r2_score()
    - 'r2'
        - 𝑦𝑖  : i번째 실제 값,

        - 𝑦𝑖^  : i 번째 예측 값,

        - 𝑦¯  : y의 평균

 ### Guide
  - 결정계수 . 바이오에선 90%, 공학에서는 70%, 사회과학에선 13% 정도가 기준이 된다고 한다
  - 
    ``` python
        import numpy as np
        import matplotlib.pyplot as plt
        import seaborn as sns
        
        from sklearn.datasets import make_regression
        from sklearn.model_selection impmort cross_val_score
        from sklearn.linear_model import LinearRegression
        
        X,y = make_regression(n_samples = 100,
                              n_features = 1,
                              n_informative = 1,
                              noise = 50,
                              coef = False,
                              random_state = 1)
                              
        lr = LinearRegression()
        lr.fit(X,y)
        
        pred = lr.predict(X)
        
        from sklearn.metrics import mean_squared_error, r2score, mean_absolute_error
        
        mse = mean_square_error(y,pred)
        r2 = r2_score(y,pred)
        mae = mean_absolute_error(y,pred)
        
        score = cross_val_score(lr, X,y, cv=5)
        np.mean(score)
        
        cross_val_score(lr,X,y,scoring = 'neg_mean_squared_error',cv = 5)*-1
        # 오차점수의 경우 낮을수록 성능이 좋은것인데 양수로 계산하게 되면 정렬이 내림차순으로 되기 때문에 음수로 나오게 한다
        
        lr.coef_, lr.intercept_ # 가중치와 절편값
        
        plt.scatter(X,y,label='truth')
        y_hat = X*lr.coef + lr.intercept_
        plt.plot(X,y_hat, color = 'red',label='prediction')
        plt.legend()
    ```
    
 #### 기존 분류 모델과 회귀 모델
 
 - 
  ``` python'
      from sklearn.model_selection import train_test_split
      from sklearn.neighbors import KNeighborsRegressor
      from sklearn.tree import DecisionTreeRegressor
      from sklearn.ensemble import RandomForestRegressor, VotingRegressor
      from sklearn.linear_model import LinearRegression
      
      X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2, random_state=12) # stratify는 분류에서만 지정해주고 회귀에서는 X
      
      knn_reg = KNeighborsRegressor(n_neighbors=3)
      tree_reg = DecisionTreeRegressor(max_depth=5)
      rf_reg = RandomForestRegressor(n_estimators = 300, max_depth = 2)
      lr_reg = LinearRegression()
      
      estimators = [('knn',knn_reg},('tree',tree_reg),('random forest',rf_reg)]
      
      def print_metrics(y,y_pred, title=None):
        mse = mean_squared_error(y,y_pred)
        rmse = np.sqrt(mse)
        r2 = r2_score(y,y_pred)
        if title:
          print(title)
        print(f'MSE:{mse}, RMSE:{rmse}, R2:{r2}')
        
       r2_list = []
       mse_list = []
       
       for name, model in estimators:
          model.fit(X_train,y_train)
          pred_train = model.predict(X_train)
          pred_test = model.predict(X_test)
          
          print_metrics(y_train,pred_train, name+' -Train')
          print_metrics(y_test,pred_test, name+' -Test')
          print('---------------------------------------')
          
       #VotingRegressor

       vote_reg = VotingRegressor(estimators)
       vote_reg.fit(X_train,y_train)

       pred_train = vote_reg.predict(X_train)
       pred_test = vote_reg.predict(X_test)

       print_metrics(y_train,pred_train)
       print_metrics(y_test, pred_test)
         
       tree_reg = DecisionTreeRegressor(max_depth=3)
       
       tree_reg.fit(X_train,y_train)
       
       from sklearn.tree import export_graphviz
       from graphviz import Source
       
       graph = Source(export_graphviz(tree_reg,
                                      out_file = None,
                                      rounded = True,
                                      filled = True))
   ```
      








