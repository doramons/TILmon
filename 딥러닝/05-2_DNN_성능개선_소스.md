## 모델의 크기 변경

``` python
    # 작은 모델 => layer층, units수가 적은 것
    def create_model():
        model = keras.Sequential()
        model.add(keras.layers.Input((IMAGE_SIZE,IMAGE_SIZE))
        model.add(keras.layers.Flatten())
        
        # hidden
        model.add(keras.layers.Dense(8, activation='relu'))
        
        # output
        model.add(keras.layers.Dense(N_CLASS, activation='softmax'))
        
        model.compile(optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE),loss='categorical_crossentropy',metrics=['accuracy'])
        
        return model
        
     model = model.create_model()
     model.summary()
     
     history = model.fit(train_dataset,
                         epochs = N_EPOCHS,
                         steps_per_epoch = steps_per_epochs,
                         validation_data = val_dataset,
                         validation_steps = validation_steps)
     plot_result(history)
 ```
  ![image](https://user-images.githubusercontent.com/76146752/115634270-3e1b1d00-a344-11eb-83ed-be6780f0c8ef.png)

  ![image](https://user-images.githubusercontent.com/76146752/115574010-36378a80-a2fc-11eb-913a-bff3b4a988c4.png)

``` python
    # 큰 모델
    def create_model():
        model = keras.Sequential()
        model.add(keras.layers.Input((IMAGE_SIZE,IMAGE_SIZE))
        model.add(keras.layers.Flatten())

        # hidden
        model.add(keras.layers.Dense(256,activation='relu'))
        model.add(keras.layers.Dense(256,activation='relu'))
        model.add(keras.layers.Dense(128, activation='relu'))
        model.add(keras.layers.Dense(128,activation='relu'))

        # Output
        model.add(keras.layers.Dense(N_CLASS, activation='softmax))

        model.compile(optimizer=keras.optimizers.Adam(learning_rate = LEARNING_RATE),
                                                      loss = 'categorical_crossentroy',
                                                      metrics = ['accuracy'])

    # 학습
    history = model.fit(train_dataset,
                        epochs=N_EPOCHS, 
                        steps_per_epoch = steps_per_epoch, 
                        validation_data= val_dataset, 
                        validation_steps = validation)
``` 
  
 ![image](https://user-images.githubusercontent.com/76146752/115575424-82cf9580-a2fd-11eb-9104-7088a2368483.png)


### Dropout 적용

  - dropout layer는 적용하려는 layer 앞에 추가한다
  - dropout 비율은 0~1 사이 실수로 지정하는데 보통 0.2 ~ 0.5 값을 지정한다
  - dropout이 적용된 모델을 학습시킬 때는 epoch 수를 더 늘려준다

``` python
    DROPOUT_RATE = 0.5
    def create_dropout_model():
        model = keras.Sequential()
        model.add(keras.layers.Input((IMAGE_SIZE,IMAGE_SIZE))
        model.add(keras.layers.Flatten())
        
        model.add(keras.layers.Dropout(rate=DROPOUT_RATE))
        model.add(keras.layers.Dense(256, activation='relu))
        
        model.add(keras.layers.Dropout(rate=DROPOUT_RATE))
        model.add(keras.layers.Dense(256, activation='relu))
        
        model.add(keras.layers.Dropout(rate=DROPOUT_RATE))
        model.add(keras.layers.Dense(128, activation='relu))
        
        model.add(keras.layers.Dropout(rate=DROPOUT_RATE))
        model.add(keras.layers.Dense(128, activation='relu))
        
        model.add(keras.layers.Dropout(rate=DROPOUT_RATE))
        model.add(keras.layers.Dense(N_CLASS, activation='softmax'))
        model.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),loss='categorical_crossentropy',metrics=['accuracy'])
        
        return model
        
     model = create_dropout_model()
     
     plot_model(model,show_shapes=True)
```
![image](https://user-images.githubusercontent.com/76146752/115634767-38720700-a345-11eb-9b52-893471c86b55.png)

``` python
    history = model.fit(train_dataset,
                        epoch = N_EPOCHS,
                        steps_per_epoch = steps_per_epoch,
                        validation_data = val_dataset,
                        validation_steps = validation_steps)
                        
     plot_result(history)
     
     model.evaluate(val_dataset)
 ```
 ![image](https://user-images.githubusercontent.com/76146752/115634906-81c25680-a345-11eb-936a-5fdae839e2f0.png)


### Batch Normalization

``` python
    def create_BN_model():
        model = keras.Sequential()
        model.add(keras.layers.Input((IMAGE_SIZE, IMAGE_SIZE))
        model.add(keras.layers.Flatten())
        
        model.add(keras.layers.Dense(256)) # 완전연결층 (활성화함수는 설정 x)
        model.add(keras.layers.BatchNormalization()) #활성화함수 전 배치정규화
        model.add(keras.layers.ReLU()) # 활성화함수
        
        model.add(keras.layers.Dense(256))
        model.add(keras.layers.BatchNormalization())
        model.add(keras.layers.ReLU())
        
        model.add(keras.layers.Dense(128))
        model.add(keras.layers.BatchNormalization())
        model.add(keras.layers.ReLU()
        
        model.add(keras.layers.Dense(128))
        model.add(keras.layers.BatchNormalization())
        model.add(keras.layers.ReLU())
        
        model.add(keras.layers.Dense(N_CLASS))
        model.add(keras.layers.BatchNormalization())
        model.add(keras.layers.Softmax())
        
        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
        
        return model
        
    plot_result(history)
```
![image](https://user-images.githubusercontent.com/76146752/115635717-4aed4000-a347-11eb-8b13-324142e3b1c8.png)

### 학습률 조절

``` python
    LEARNING_RATE = 0.001
    N_EPOCHS = 30
    
    def create_model():
        model = keras.Seqeuntial()
        model.add(keras.layers.Input((IMAGE_SIZE,IMAGE_SIZE))
        model.add(keras.layers.Flatten())
        
        model.add(keras.layers.Dense(256, activation='relu'))
        model.add(keras.layers.Dense(128, activation='relu'))
        model.add(keras.layers.Dense(128, activation='relu'))
        model.add(keras.layers.Dense(N_CLASS, activation='softmax'))
        
        return model
        
  ```
    
 ### ExpoenetialDecay 를 사용
   - 일정한 step마다 일정한 비율로 학습률을 변경(줄여준다)

``` python
    lr_scheduler = keras.optimizers.schedules.ExponentialDecay(
                         initial_learning_rate = LEARNING_RATE, # 시작학습률
                         decay_steps = steps_per_epoch*10, # 몇 step마다 학습률을 변경시킬 것인지(10에폭마다 학습률 줄이겠다)
                         decay_rate = 0.5, # 학습률의 변화율, 기존학습률*지정한 값
                         staircase = True # True: 한 번에 변경, False:매 스텝마다 조금씩 변화
        
        
    ) # optimizer의 학습률 대신 lr_scheduler를 설정
    
    model.compile(optimizer=keras.optimizers.Adam(learning_rate =lr_scheduler),loss='categorical_crossentropy',metrics=['accuracy'])
    
```

### LearningRate 조정과 관련 callback 사용
   - callback: 학습도중 이벤트(변화)가 발생하면 호출되는 기능
 
 #### ReduceLROnPlateau callback
   - patience 에폭동안 monitor에 지정한 지표가 향상되지 않으면 현재 learning_rate에 factor(0~1 실수)를 곱해서 learning rate를 조정한다

``` python
    # ReduceLROnPlateau callback 생성
    rlp_callback = keras.callbacks.ReduceLROnPlateau(patience=10, # 10에폭동안 기다려라
                                                     monitor = 'val_loss', # validation loss의 지표가 향상되지 않으면
                                                     factor = 0.5, #현재 학습률에 0.5 곱한 ReduceLROnPlateau callback 생성
                                                     verbose = 1) # 학습률이 바뀌면 로그 출력
    
    
    # Callback은 fit 할 때 적용
    history = model.fit(train_dataset,
                        epochs = N_EPOCHS,
                        steps_per_epoch = steps_per_epoch,
                        validation_data = val_dataset,
                        validation_steps = validation_steps,
                        callback = [rlp_callback])
                        
```                        
 
 #### LearningRateScheduler callback 사용
   - 사용자 정의 학습률 조정함수를 만들어 적용
   - 학습률 조정하는 함수를 만들어서 callback 생성시 등록
        - 매개변수
            1. epoch
            2. 현시점 learning rate
        - 반환값
            - 조정한 learning rate

``` python
    def user_lr(epoch, lr):
        if epoch < 5:
            return lr
            
        elif epoch < 10:
            if epoch == 6:
                print('======change Learning Rate: ', lr*0.5, 'EPOCHS :', epoch)
            return lr *0.5
            
         elif epoch < 20:
            if epooch == 11:
                print('======change Learning Rate: ', lr*0.5, 'EPOCHS :',epoch)
                
         elif epoch < 30:
            if epoch == 21:
                print('======change Learning Rate: ', lr*0.5, 'EPOCHS :', epoch)
                
             return lr*0.5
             
          else:
            return lr
```

``` python
    lr_scheduler_callback = keras.callbacks.LearningRateScheduler(user_lr)
    
    model = create_model()
    model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy']) # learningrate 설정x
    
    history = model.fit(train_dataset,epochs=N_EPOCHS,
                        steps_per_epoch = steps_per_epoch,
                        validation_data = val_dataset,
                        validation_steps = validation_steps,
                        callbacks = [lr_scheduler_callback])
```
    
