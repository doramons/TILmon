## í™œì„± í•¨ìˆ˜(Activation Function)
  - ê° ìœ ë‹›ì´ ì…ë ¥ê²°ê³¼ë¥¼ ì²˜ë¦¬ê²°ê³¼ë¥¼ ì²˜ë¦¬í•œ í›„ ì¶œë ¥í•˜ê¸° ìœ„í•´ ê±°ì§€ëŠ” í•¨ìˆ˜
  - ê°™ì€ ì¸µ(layer)ì˜ ëª¨ë“  ìœ ë‹›ë“¤ì€ ê°™ì€ í™œì„± í•¨ìˆ˜ë¥¼ ê°€ì§„ë‹¤
  - ìµœì¢… **ì¶œë ¥ ë ˆì´ì–´ì˜ ê²½ìš° ë¬¸ì œìœ í˜•ì— ë”°ë¥¸ í‘œì¤€ í™œì„±í™” í•¨ìˆ˜ê°€ ì¡´ì¬í•œë‹¤.**
  - ì€ë‹‰ì¸µ (Hidden Layer)ì˜ ê²½ìš° **ReLU** í•¨ìˆ˜ë¥¼ ì£¼ë¡œ ì‚¬ìš©í•œë‹¤.

### ì£¼ìš” í™œì„±í•¨ìˆ˜(Activation Function)
  - ### Sigmoid (logistic function)
![image](https://user-images.githubusercontent.com/76146752/115180474-9e297d80-a110-11eb-8ee5-b895a1260a5c.png)
    
   - 0 < ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘(ğ‘§) < 1
   - í•œê³„
   - ### Binary classification(ì´ì§„ ë¶„ë¥˜)ë¥¼ ìœ„í•œ ë„¤íŠ¸ì›Œí¬ì˜ Output layer(ì¶œë ¥ì¸µ)ì˜ í™œì„±í•¨ìˆ˜ë¡œ ì‚¬ìš©ëœë‹¤
      - ìœ„ì™€ ê°™ì€ í•œê³„ ë•Œë¬¸ì— hidden layer(ì€ë‹‰ì¸µ)ì˜ í™œì„±í•¨ìˆ˜ë¡œëŠ” ì˜ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ”ë‹¤
      
      > ### ê¸°ìš¸ê¸° ì†Œì‹¤(Gradient Vanishing)
      >   - ìµœì í™” ê³¼ì •ì—ì„œ gradientê°€ 0ê³¼ ë°‘ë‹¨ì¸µ(Bottom Layer)ì˜ ê°€ì¤‘ì¹˜ë“¤ì´ í•™ìŠµì´ ì•ˆë˜ëŠ” í˜„ìƒ
      >   
   - ### Hypterbolic tangent
  ![image](https://user-images.githubusercontent.com/76146752/115180745-51927200-a111-11eb-8a0a-12339877e998.png)
    - âˆ’1 < ğ‘¡ğ‘ğ‘›â„(ğ‘§) < 1
    - Outputì´ 0ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë¶„í¬í•˜ë¯€ë¡œ sigmoidë³´ë‹¤ í•™ìŠµì— íš¨ìœ¨ì ì´ë‹¤
    - ì—¬ì „íˆ ê¸°ìš¸ê¸° ì†Œì‹¤(Graident Vanishing) ë¬¸ì œë¥¼ ë°œìƒì‹œí‚¨ë‹¤
    - ì´ì§„ë¶„ë¥˜ ì¶œë ¥ì¸µì— ì“°ê³¤í•¨(ex. gan ëª¨ë¸)

   - ### ReLU(Tectified Linear Unit)
  ![image](https://user-images.githubusercontent.com/76146752/115180894-a59d5680-a111-11eb-948f-c4f28ce1366b.png)
      - ğ‘…ğ‘’ğ¿ğ‘ˆ(ğ‘§)=ğ‘šğ‘ğ‘¥(0,ğ‘§)
      - ê¸°ìš¸ê¸° ì†Œì‹¤(Gradient Vanishing) ë¬¸ì œë¥¼ ì–´ëŠì •ë„ í•´ê²°
      - 0 ì´í•˜ì˜ ê°’(z<=0)ë“¤ì— ëŒ€í•´ ë‰´ëŸ°ì´ ì£½ëŠ” ë‹¨ì ì´ ìˆë‹¤(Dying ReLU)

   - ### Leaky ReLU
   ![image](https://user-images.githubusercontent.com/76146752/115181010-e85f2e80-a111-11eb-846c-8b9871b98f22.png)
    - ğ¿ğ‘’ğ‘ğ‘˜ğ‘¦ğ‘…ğ‘’ğ¿ğ‘ˆ(ğ‘§)=ğ‘šğ‘ğ‘¥(ğ›¼ğ‘§,ğ‘§)
    - 0 < ğ›¼ < 1
    - ReLUì˜ Dying ReLU í˜„ìƒì„ í•´ê²°í•˜ê¸° ìœ„í•´ ë‚˜ì˜¨ í•¨ìˆ˜ - ìŒìˆ˜ zë¥¼ 0ìœ¼ë¡œ ë°˜í™˜í•˜ì§€ ì•Šê³  alpha (0~1 ì‚¬ì´ ì‹¤ìˆ˜)ë¥¼ ê³±í•´ ë°˜í™˜í•œë‹¤

   - ### Softmax
        - ğ‘†ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘§ğ‘—) = ğ‘’ğ‘¥ğ‘(ğ‘§ğ‘—)/âˆ‘ğ¾ğ‘˜=1ğ‘’ğ‘¥ğ‘(ğ‘§ğ‘˜)
        -  ğ‘—=1,2,â€¦,ğ¾
      -  **Multi-class classification(ë‹¤ì¤‘ë¶„ë¥˜)ë¥¼ ìœ„í•œ ë„¤íŠ¸ì›Œí¬ì˜ Output Layer(ì¶œë ¥ì¸µ)ì˜ í™œì„±í•¨ìˆ˜ë¡œ ì‚¬ìš©ëœë‹¤ **
        - ì€ë‹‰ì¸µì˜ í™œì„±í•¨ìˆ˜ë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤
      -  ê° classì˜ scoreë¥¼ ì •ê·œí™” í•˜ì—¬ ê° classì— ëŒ€í•œ í™•ë¥  ê°’ìœ¼ë¡œ ë³€í™˜
          - ì¶œë ¥ ë…¸ë“œë“¤ì˜ ê°’ì€ 0~1 ì‚¬ì´ì˜ ì‹¤ìˆ˜ë¡œ ë³€í™˜ë˜ê³  ê·¸ ê°’ì˜ ì´ í•©ì€ 1ì´ëœë‹¤

![image](https://user-images.githubusercontent.com/76146752/115213617-a8617100-a13c-11eb-84c4-d3249673e764.png)

![image](https://user-images.githubusercontent.com/76146752/115213633-ac8d8e80-a13c-11eb-81e1-a61dff3a17e1.png)

### Optimizer(ìµœì í™” ë°©ë²•)
  - Loss functionì„ ê¸°ë°˜ìœ¼ë¡œ ë„¤íŠ¸ì›Œí¬ê°€ ì–´ë–»ê²Œ ì—…ë°ì´íŠ¸ ë ì§€ë¥¼ ê²°ì •í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜
     - ê²½ì‚¬í•˜ê°•ë²•ê³¼ ì˜¤ì°¨ ì—­ì „íŒŒ(back propagation) ì•Œê³ ë¦¬ì¦˜ì„ ì´ìš©í•´ weightë¥¼ ìµœì í™” í•œë‹¤


#### Gradient Decent(ê²½ì‚¬í•˜ê°•ë²•)
  - ### ìµœì í™”
    - ëª¨ë¸(ë„¤íŠ¸ì›Œí¬)ê°€ ì¶œë ¥í•œ ê²°ê³¼ì™€ ì‹¤ì œê°’(Ground Truth)ì˜ ì°¨ì´ë¥¼ ì •ì˜í•˜ëŠ” í•¨ìˆ˜ë¥¼ **Loss function(ì†ì‹¤í•¨ìˆ˜, ë¹„ìš©í•¨ìˆ˜)** ë¼ê³  í•œë‹¤
    - Train ì‹œ Loss functionì´ ì¶œë ¥í•˜ëŠ” ê°’ì„ ì¤„ì´ê¸° ìœ„í•´ íŒŒë¼ë¯¸í„°(weight, bias)ë¥¼ update ê³¼ì •ì„ ìµœì í™” (Optimization)ì´ë¼ê³  í•œë‹¤
  - Gradient Decent(ê²½ì‚¬í•˜ê°•ë²•)
    - ìµœì í™”ë¥¼ ìœ„í•´ íŒŒë¼ë¯¸í„°ë“¤ì— ëŒ€í•œ loss functionì˜ Gradientê°’ì„ êµ¬í•´ Gradientì˜ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ì¼ì • í¬ê¸°ë§Œí¼ íŒŒë¼ë¯¸í„°ë“¤ì„ ì—…ë°ì´íŠ¸ í•˜ëŠ” ê²ƒì„ ê²½ì‚¬í•˜ê°•ë²•ì´ë¼ê³  í•œë‹¤  í•œ

#### íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ë‹¨ìœ„
  - Batch Gradient Decent(ë°°ì¹˜ ê²½ì‚¬í•˜ê°•ë²•)
  - Mini Batch Stochastic Gradient Deccent (ë¯¸ë‹ˆ ë°°ì¹˜ í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•)











