## 활성 함수(Activation Function)
  - 각 유닛이 입력결과를 처리결과를 처리한 후 출력하기 위해 거지는 함수
  - 같은 층(layer)의 모든 유닛들은 같은 활성 함수를 가진다
  - 최종 **출력 레이어의 경우 문제유형에 따른 표준 활성화 함수가 존재한다.**
  - 은닉층 (Hidden Layer)의 경우 **ReLU** 함수를 주로 사용한다.

### 주요 활성함수(Activation Function)
  - ### Sigmoid (logistic function)
![image](https://user-images.githubusercontent.com/76146752/115180474-9e297d80-a110-11eb-8ee5-b895a1260a5c.png)
    
   - 0 < 𝑠𝑖𝑔𝑚𝑜𝑖𝑑(𝑧) < 1
   - 한계
   - ### Binary classification(이진 분류)를 위한 네트워크의 Output layer(출력층)의 활성함수로 사용된다
      - 위와 같은 한계 때문에 hidden layer(은닉층)의 활성함수로는 잘 사용되지 않는다
      
      > ### 기울기 소실(Gradient Vanishing)
      >   - 최적화 과정에서 gradient가 0과 밑단층(Bottom Layer)의 가중치들이 학습이 안되는 현상
      >   
   - ### Hypterbolic tangent
  ![image](https://user-images.githubusercontent.com/76146752/115180745-51927200-a111-11eb-8a0a-12339877e998.png)
    - −1 < 𝑡𝑎𝑛ℎ(𝑧) < 1
    - Output이 0을 중심으로 분포하므로 sigmoid보다 학습에 효율적이다
    - 여전히 기울기 소실(Graident Vanishing) 문제를 발생시킨다
    - 이진분류 출력층에 쓰곤함(ex. gan 모델)

   - ### ReLU(Tectified Linear Unit)
  ![image](https://user-images.githubusercontent.com/76146752/115180894-a59d5680-a111-11eb-948f-c4f28ce1366b.png)
      - 𝑅𝑒𝐿𝑈(𝑧)=𝑚𝑎𝑥(0,𝑧)
      - 기울기 소실(Gradient Vanishing) 문제를 어느정도 해결
      - 0 이하의 값(z<=0)들에 대해 뉴런이 죽는 단점이 있다(Dying ReLU)

   - ### Leaky ReLU
   ![image](https://user-images.githubusercontent.com/76146752/115181010-e85f2e80-a111-11eb-846c-8b9871b98f22.png)
    - 𝐿𝑒𝑎𝑘𝑦𝑅𝑒𝐿𝑈(𝑧)=𝑚𝑎𝑥(𝛼𝑧,𝑧)
    - 0 < 𝛼 < 1
    - ReLU의 Dying ReLU 현상을 해결하기 위해 나온 함수 - 음수 z를 0으로 반환하지 않고 alpha (0~1 사이 실수)를 곱해 반환한다

   - ### Softmax
        - 𝑆𝑜𝑓𝑡𝑚𝑎𝑥(𝑧𝑗) = 𝑒𝑥𝑝(𝑧𝑗)/∑𝐾𝑘=1𝑒𝑥𝑝(𝑧𝑘)
        -  𝑗=1,2,…,𝐾
      -  **Multi-class classification(다중분류)를 위한 네트워크의 Output Layer(출력층)의 활성함수로 사용된다 **
        - 은닉층의 활성함수로 사용하지 않는다
      -  각 class의 score를 정규화 하여 각 class에 대한 확률 값으로 변환
          - 출력 노드들의 값은 0~1 사이의 실수로 변환되고 그 값의 총 합은 1이된다

![image](https://user-images.githubusercontent.com/76146752/115213617-a8617100-a13c-11eb-84c4-d3249673e764.png)

![image](https://user-images.githubusercontent.com/76146752/115213633-ac8d8e80-a13c-11eb-81e1-a61dff3a17e1.png)

### Optimizer(최적화 방법)
  - Loss function을 기반으로 네트워크가 어떻게 업데이트 될지를 결정하는 알고리즘
     - 경사하강법과 오차 역전파(back propagation) 알고리즘을 이용해 weight를 최적화 한다


#### Gradient Decent(경사하강법)
  - ### 최적화
    - 모델(네트워크)가 출력한 결과와 실제값(Ground Truth)의 차이를 정의하는 함수를 **Loss function(손실함수, 비용함수)** 라고 한다
    - Train 시 Loss function이 출력하는 값을 줄이기 위해 파라미터(weight, bias)를 update 과정을 최적화 (Optimization)이라고 한다
  - Gradient Decent(경사하강법)
    - 최적화를 위해 파라미터들에 대한 loss function의 Gradient값을 구해 Gradient의 반대 방향으로 일정 크기만큼 파라미터들을 업데이트 하는 것을 경사하강법이라고 한다  한

#### 파라미터 업데이트 단위
  - Batch Gradient Decent(배치 경사하강법)
  - Mini Batch Stochastic Gradient Deccent (미니 배치 확률적 경사하강법)











